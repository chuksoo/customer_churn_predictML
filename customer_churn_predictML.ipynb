{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lZ_-ql1zRCIE"
   },
   "source": [
    "# Predicting Customer Churn with Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pwaUd1PSRCIH"
   },
   "source": [
    "Beta Bank customers are leaving: little by little, chipping away every month. The bankers figured out it’s cheaper to save the existing customers rather than to attract new ones.\n",
    "We need to predict whether a customer will leave the bank soon. You have the data on clients’ past behavior and termination of contracts with the bank. \n",
    "\n",
    "Build a model with the maximum possible F1 score. To pass the project, you need an F1 score of at least 0.59. Check the F1 for the test set. Additionally, measure the AUC-ROC metric and compare it with the F1.\n",
    "Data source: https://code.s3.yandex.net/datasets/Churn.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S5Qu0KCQRCII"
   },
   "source": [
    "##### Business Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2kCNm5ECRCII"
   },
   "source": [
    "One of the key business metrics (along with cash flow etc.) for banks, internet providers, pay TV companies, telecom companies is customer attrition analysis (or customer churn analysis). We say customer churn is loss of clients or customers. It is cheaper to keep existing customers than go for new ones. Beta bank have already seen the effect of customer churn as it affects their end of the year revenue and monthly recurring revenue (or MRR). To this end, we need to predict whether a customer will leave a bank soon given their past relationship and behavior while operating with Beta bank. The bank hopes to deploy churn prediction models and effective retention strategies in managing customer attrition thereby preventing significant loss of revenue from defecting customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nBw7aKf0RCIJ"
   },
   "source": [
    "##### Task Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YJDHUmFYRCIJ"
   },
   "source": [
    "Using the customer data, train a model that predicts whether a customer will leave the bank soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mNXobDuYRCIJ"
   },
   "source": [
    "## Data description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J0v2AEkrRCIJ"
   },
   "source": [
    "The data can be found in '/datasets/Churn.csv' file. Download the dataset. \n",
    "\n",
    "**Features**\n",
    "\n",
    " - `RowNumber` — data string index\n",
    " - `CustomerId` — unique customer identifier\n",
    " - `Surname` — surname\n",
    " - `CreditScore` — credit score\n",
    " - `Geography` — country of residence\n",
    " - `Gender` — gender\n",
    " - `Age` — age\n",
    " - `Tenure` — period of maturation for a customer’s fixed deposit (years)\n",
    " - `Balance` — account balance\n",
    " - `NumOfProducts` — number of banking products used by the customer\n",
    " - `HasCrCard` — customer has a credit card\n",
    " - `IsActiveMember` — customer’s activeness\n",
    " - `EstimatedSalary` — estimated salary\n",
    "\n",
    "**Target**\n",
    "\n",
    " - `Exited` — сustomer has left"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lxYpiMGxRCIK"
   },
   "source": [
    "## Objectives\n",
    "\n",
    "The objectives of this project is to:\n",
    "- Develop a model that would predicts whether a customer will leave the bank soon\n",
    "- Build a machine learning model with the maximum possible F1 score of atleast 0.59 or higher.\n",
    "- Measure the AUC-ROC metric and compare it with the F1 score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RcFqyhd8RCIK"
   },
   "source": [
    "<hr>\n",
    "\n",
    " # Table of contents\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "    <ol>\n",
    "        <li><a href=\"#open_the_data\">Open the data file and study the general information</a></li>\n",
    "        <li><a href=\"#data_preparation\">Prepare the data</a></li>\n",
    "        <li><a href=\"#feature_engineering\">Feature engineering</a></li>\n",
    "        <li><a href=\"#class_balance\">Examine the balance of classes</a></li>\n",
    "        <li><a href=\"#improve_quality\">Improve the quality of the model</a></li>\n",
    "        <li><a href=\"#investigate_models\">Investigate different models quality</a></li>\n",
    "        <li><a href=\"#check_quality\">Check model quality</a></li>\n",
    "        <li><a href=\"#overall_conclusion\">Overall conclusion</a></li>\n",
    "    </ol>\n",
    "</div>\n",
    "<br>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J0N1vpRFRCIL"
   },
   "source": [
    "<div id=\"open_the_data\">\n",
    "    <h2>Open the data file and study the general information</h2> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "42v9vA68RCIL"
   },
   "source": [
    "We require the following libraries: *pandas* and *numpy* for data preprocessing and manipulation, *Scikit-Learn* for building our learning algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nxat2ScqRCIL",
    "outputId": "5e3bc02e-3468-4711-cc80-8ceae780515b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project libraries has been successfully been imported!\n"
     ]
    }
   ],
   "source": [
    "# import pandas and numpy for data preprocessing and manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# matplotlib for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# import train_test_split to split data\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "pd.options.mode.chained_assignment = None # to avoid SettingWithCopyWarning after scaling\n",
    "\n",
    "# import machine learning module from the sklearn library\n",
    "from sklearn.dummy import DummyClassifier # import dummy classifier\n",
    "from sklearn.tree import DecisionTreeClassifier # import decision tree classifier\n",
    "from sklearn.linear_model import LogisticRegression # import logistic regression \n",
    "from sklearn.ensemble import RandomForestClassifier # import random forest algorithm\n",
    "from sklearn.ensemble import AdaBoostClassifier # import adaboost classifier algorithm\n",
    "from catboost import CatBoostClassifier # import catboost classifier\n",
    "\n",
    "# import metrics for sanity check on model\n",
    "from sklearn.metrics import *\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import balanced_accuracy_score, roc_auc_score\n",
    "\n",
    "# import warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# import sklearn utilities\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "print('Project libraries has been successfully been imported!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JqDunOaHRCIN",
    "outputId": "049e0508-d519-4eb8-9043-1d393e9ede99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been read correctly!\n"
     ]
    }
   ],
   "source": [
    "# read the data\n",
    "try:\n",
    "    df = pd.read_csv('https://code.s3.yandex.net/datasets/Churn.csv')\n",
    "except:\n",
    "    df = pd.read_csv('C:/Users/hotty/Desktop/Practicum by Yandex/Projects/Supervised Learning/Churn.csv')\n",
    "print('Data has been read correctly!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BETUVWd_RCIN"
   },
   "outputs": [],
   "source": [
    "# function to determine if columns in file have null values\n",
    "def get_percent_of_na(df, num):\n",
    "    count = 0\n",
    "    df = df.copy()\n",
    "    s = (df.isna().sum() / df.shape[0])\n",
    "    for column, percent in zip(s.index, s.values):\n",
    "        num_of_nulls = df[column].isna().sum()\n",
    "        if num_of_nulls == 0:\n",
    "            continue\n",
    "        else:\n",
    "            count += 1\n",
    "        print('Column {} has {:.{}%} percent of Nulls, and {} of nulls'.format(column, percent, num, num_of_nulls))\n",
    "    if count != 0:\n",
    "        print(\"\\033[1m\" + 'There are {} columns with NA.'.format(count) + \"\\033[0m\")\n",
    "    else:\n",
    "        print()\n",
    "        print(\"\\033[1m\" + 'There are no columns with NA.' + \"\\033[0m\")\n",
    "        \n",
    "# function to display general information about the dataset\n",
    "def get_info(df):\n",
    "    \"\"\"\n",
    "    This function uses the head(), info(), describe(), shape() and duplicated() \n",
    "    methods to display the general information about the dataset.\n",
    "    \"\"\"\n",
    "    print(\"\\033[1m\" + '-'*100 + \"\\033[0m\")\n",
    "    print('Head:')\n",
    "    print()\n",
    "    display(df.head())\n",
    "    print('-'*100)\n",
    "    print('Info:')\n",
    "    print()\n",
    "    display(df.info())\n",
    "    print('-'*100)\n",
    "    print('Describe:')\n",
    "    print()\n",
    "    display(df.describe())\n",
    "    print('-'*100)\n",
    "    display(df.describe(include='object'))\n",
    "    print()\n",
    "    print('Columns with nulls:')\n",
    "    display(get_percent_of_na(df, 4))  # check this out\n",
    "    print('-'*100)\n",
    "    print('Shape:')\n",
    "    print(df.shape)\n",
    "    print('-'*100)\n",
    "    print('Duplicated:')\n",
    "    print(\"\\033[1m\" + 'We have {} duplicated rows.\\n'.format(df.duplicated().sum()) + \"\\033[0m\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "L5J30KZmRCIO",
    "outputId": "9b7f095e-a4a3-43ca-c85e-c17e52f0029b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "General information about the dataframe\n",
      "\u001b[1m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "Head:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1.0</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8.0</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2.0</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n",
       "0          1    15634602  Hargrave          619    France  Female   42   \n",
       "1          2    15647311      Hill          608     Spain  Female   41   \n",
       "2          3    15619304      Onio          502    France  Female   42   \n",
       "3          4    15701354      Boni          699    France  Female   39   \n",
       "4          5    15737888  Mitchell          850     Spain  Female   43   \n",
       "\n",
       "   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "0     2.0       0.00              1          1               1   \n",
       "1     1.0   83807.86              1          0               1   \n",
       "2     8.0  159660.80              3          1               0   \n",
       "3     1.0       0.00              2          0               0   \n",
       "4     2.0  125510.82              1          1               1   \n",
       "\n",
       "   EstimatedSalary  Exited  \n",
       "0        101348.88       1  \n",
       "1        112542.58       0  \n",
       "2        113931.57       1  \n",
       "3         93826.63       0  \n",
       "4         79084.10       0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Info:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 14 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   RowNumber        10000 non-null  int64  \n",
      " 1   CustomerId       10000 non-null  int64  \n",
      " 2   Surname          10000 non-null  object \n",
      " 3   CreditScore      10000 non-null  int64  \n",
      " 4   Geography        10000 non-null  object \n",
      " 5   Gender           10000 non-null  object \n",
      " 6   Age              10000 non-null  int64  \n",
      " 7   Tenure           9091 non-null   float64\n",
      " 8   Balance          10000 non-null  float64\n",
      " 9   NumOfProducts    10000 non-null  int64  \n",
      " 10  HasCrCard        10000 non-null  int64  \n",
      " 11  IsActiveMember   10000 non-null  int64  \n",
      " 12  EstimatedSalary  10000 non-null  float64\n",
      " 13  Exited           10000 non-null  int64  \n",
      "dtypes: float64(3), int64(8), object(3)\n",
      "memory usage: 1.1+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Describe:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.00000</td>\n",
       "      <td>1.000000e+04</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>9091.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.00000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5000.50000</td>\n",
       "      <td>1.569094e+07</td>\n",
       "      <td>650.528800</td>\n",
       "      <td>38.921800</td>\n",
       "      <td>4.997690</td>\n",
       "      <td>76485.889288</td>\n",
       "      <td>1.530200</td>\n",
       "      <td>0.70550</td>\n",
       "      <td>0.515100</td>\n",
       "      <td>100090.239881</td>\n",
       "      <td>0.203700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2886.89568</td>\n",
       "      <td>7.193619e+04</td>\n",
       "      <td>96.653299</td>\n",
       "      <td>10.487806</td>\n",
       "      <td>2.894723</td>\n",
       "      <td>62397.405202</td>\n",
       "      <td>0.581654</td>\n",
       "      <td>0.45584</td>\n",
       "      <td>0.499797</td>\n",
       "      <td>57510.492818</td>\n",
       "      <td>0.402769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.556570e+07</td>\n",
       "      <td>350.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.580000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2500.75000</td>\n",
       "      <td>1.562853e+07</td>\n",
       "      <td>584.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>51002.110000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5000.50000</td>\n",
       "      <td>1.569074e+07</td>\n",
       "      <td>652.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>97198.540000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>100193.915000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7500.25000</td>\n",
       "      <td>1.575323e+07</td>\n",
       "      <td>718.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>127644.240000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>149388.247500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10000.00000</td>\n",
       "      <td>1.581569e+07</td>\n",
       "      <td>850.000000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>250898.090000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>199992.480000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         RowNumber    CustomerId   CreditScore           Age       Tenure  \\\n",
       "count  10000.00000  1.000000e+04  10000.000000  10000.000000  9091.000000   \n",
       "mean    5000.50000  1.569094e+07    650.528800     38.921800     4.997690   \n",
       "std     2886.89568  7.193619e+04     96.653299     10.487806     2.894723   \n",
       "min        1.00000  1.556570e+07    350.000000     18.000000     0.000000   \n",
       "25%     2500.75000  1.562853e+07    584.000000     32.000000     2.000000   \n",
       "50%     5000.50000  1.569074e+07    652.000000     37.000000     5.000000   \n",
       "75%     7500.25000  1.575323e+07    718.000000     44.000000     7.000000   \n",
       "max    10000.00000  1.581569e+07    850.000000     92.000000    10.000000   \n",
       "\n",
       "             Balance  NumOfProducts    HasCrCard  IsActiveMember  \\\n",
       "count   10000.000000   10000.000000  10000.00000    10000.000000   \n",
       "mean    76485.889288       1.530200      0.70550        0.515100   \n",
       "std     62397.405202       0.581654      0.45584        0.499797   \n",
       "min         0.000000       1.000000      0.00000        0.000000   \n",
       "25%         0.000000       1.000000      0.00000        0.000000   \n",
       "50%     97198.540000       1.000000      1.00000        1.000000   \n",
       "75%    127644.240000       2.000000      1.00000        1.000000   \n",
       "max    250898.090000       4.000000      1.00000        1.000000   \n",
       "\n",
       "       EstimatedSalary        Exited  \n",
       "count     10000.000000  10000.000000  \n",
       "mean     100090.239881      0.203700  \n",
       "std       57510.492818      0.402769  \n",
       "min          11.580000      0.000000  \n",
       "25%       51002.110000      0.000000  \n",
       "50%      100193.915000      0.000000  \n",
       "75%      149388.247500      0.000000  \n",
       "max      199992.480000      1.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Surname</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2932</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Smith</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>32</td>\n",
       "      <td>5014</td>\n",
       "      <td>5457</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Surname Geography Gender\n",
       "count    10000     10000  10000\n",
       "unique    2932         3      2\n",
       "top      Smith    France   Male\n",
       "freq        32      5014   5457"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Columns with nulls:\n",
      "Column Tenure has 9.0900% percent of Nulls, and 909 of nulls\n",
      "\u001b[1mThere are 1 columns with NA.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Shape:\n",
      "(10000, 14)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Duplicated:\n",
      "\u001b[1mWe have 0 duplicated rows.\n",
      "\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# study the general information about the dataset \n",
    "print('General information about the dataframe')\n",
    "get_info(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ByKFq7ZqRCIO"
   },
   "source": [
    "**Conclusion**\n",
    "\n",
    "From the information about the dataset, we have 10000 rows and 14 features. Looking at the dataset, we can see that about 9% of the data is missing in the `Tenure` column. We should also note that the missing values are *missing at random (MAR)*. To handle this missing values, we could either drop them entirely since the percentage of missing values is less than 10% or replace by the median of the column. Also, we need to correct the datatype from float to int in the `Tenure`, `Balance` and `EstimatedSalary` columns.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Y8rgNL5RCIP"
   },
   "source": [
    "<div id=\"data_preparation\">\n",
    "    <h2>Prepare the data</h2> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EZVLDz5yRCIP"
   },
   "source": [
    "#### Processing Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A75YT4caRCIP"
   },
   "source": [
    "##### Prepare `Tenure` column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qX8NPRmpRCIP"
   },
   "source": [
    "To replace missing values in the `Tenure` column, we first get the unique values of `Surname`, then get the list of possible `Tenure` for those names. We then choose a random value from the list (excluding the nan values) and assign that to the missing tenure for that surname in the dataframe. For unique surname with an empty list, we use the median of the value in the `Tenure` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "katAVk5DRCIP"
   },
   "outputs": [],
   "source": [
    "# replace missing values in the Tenure column\n",
    "# get unique values of name from this dataframe\n",
    "for surname in df['Surname'].unique().tolist():\n",
    "    # get specific 'Surname' possible Tenure\n",
    "    specific_surname_df = df[df['Surname'] == surname].dropna()['Tenure']\n",
    "    surname_tenure_list = specific_surname_df.unique().tolist()\n",
    "    # for the missing values, assign a random choice of the tenure for that surname. The default is the median of the 'Tenure'\n",
    "    if surname_tenure_list != []:\n",
    "        df.loc[(df['Surname'] == surname) & (df['Tenure'] != df['Tenure']), 'Tenure'] = random.choice(surname_tenure_list)\n",
    "    else:\n",
    "        df.loc[(df['Surname'] == surname) & (df['Tenure'] != df['Tenure']), 'Tenure'] = df['Tenure'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "khUllxrARCIQ"
   },
   "source": [
    "We have replaced missing values in the `Tenure` column based on the condition we specified. Let's look at the statistics of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "id": "vJoMJ0pLRCIQ",
    "outputId": "0eb66a9f-8f7e-4d2d-fb74-90babfe5bed8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.00000</td>\n",
       "      <td>1.000000e+04</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.00000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.00000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5000.50000</td>\n",
       "      <td>1.569094e+07</td>\n",
       "      <td>650.528800</td>\n",
       "      <td>38.921800</td>\n",
       "      <td>5.00910</td>\n",
       "      <td>76485.889288</td>\n",
       "      <td>1.530200</td>\n",
       "      <td>0.70550</td>\n",
       "      <td>0.515100</td>\n",
       "      <td>100090.239881</td>\n",
       "      <td>0.203700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2886.89568</td>\n",
       "      <td>7.193619e+04</td>\n",
       "      <td>96.653299</td>\n",
       "      <td>10.487806</td>\n",
       "      <td>2.87476</td>\n",
       "      <td>62397.405202</td>\n",
       "      <td>0.581654</td>\n",
       "      <td>0.45584</td>\n",
       "      <td>0.499797</td>\n",
       "      <td>57510.492818</td>\n",
       "      <td>0.402769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.556570e+07</td>\n",
       "      <td>350.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.580000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2500.75000</td>\n",
       "      <td>1.562853e+07</td>\n",
       "      <td>584.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>51002.110000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5000.50000</td>\n",
       "      <td>1.569074e+07</td>\n",
       "      <td>652.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>97198.540000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>100193.915000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7500.25000</td>\n",
       "      <td>1.575323e+07</td>\n",
       "      <td>718.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>7.00000</td>\n",
       "      <td>127644.240000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>149388.247500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10000.00000</td>\n",
       "      <td>1.581569e+07</td>\n",
       "      <td>850.000000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>10.00000</td>\n",
       "      <td>250898.090000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>199992.480000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         RowNumber    CustomerId   CreditScore           Age       Tenure  \\\n",
       "count  10000.00000  1.000000e+04  10000.000000  10000.000000  10000.00000   \n",
       "mean    5000.50000  1.569094e+07    650.528800     38.921800      5.00910   \n",
       "std     2886.89568  7.193619e+04     96.653299     10.487806      2.87476   \n",
       "min        1.00000  1.556570e+07    350.000000     18.000000      0.00000   \n",
       "25%     2500.75000  1.562853e+07    584.000000     32.000000      3.00000   \n",
       "50%     5000.50000  1.569074e+07    652.000000     37.000000      5.00000   \n",
       "75%     7500.25000  1.575323e+07    718.000000     44.000000      7.00000   \n",
       "max    10000.00000  1.581569e+07    850.000000     92.000000     10.00000   \n",
       "\n",
       "             Balance  NumOfProducts    HasCrCard  IsActiveMember  \\\n",
       "count   10000.000000   10000.000000  10000.00000    10000.000000   \n",
       "mean    76485.889288       1.530200      0.70550        0.515100   \n",
       "std     62397.405202       0.581654      0.45584        0.499797   \n",
       "min         0.000000       1.000000      0.00000        0.000000   \n",
       "25%         0.000000       1.000000      0.00000        0.000000   \n",
       "50%     97198.540000       1.000000      1.00000        1.000000   \n",
       "75%    127644.240000       2.000000      1.00000        1.000000   \n",
       "max    250898.090000       4.000000      1.00000        1.000000   \n",
       "\n",
       "       EstimatedSalary        Exited  \n",
       "count     10000.000000  10000.000000  \n",
       "mean     100090.239881      0.203700  \n",
       "std       57510.492818      0.402769  \n",
       "min          11.580000      0.000000  \n",
       "25%       51002.110000      0.000000  \n",
       "50%      100193.915000      0.000000  \n",
       "75%      149388.247500      0.000000  \n",
       "max      199992.480000      1.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the statistics of the new dataframe\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7kAYWdZbRCIQ",
    "outputId": "79d42563-98c9-48ed-e48c-3494db3f40cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RowNumber          0\n",
       "CustomerId         0\n",
       "Surname            0\n",
       "CreditScore        0\n",
       "Geography          0\n",
       "Gender             0\n",
       "Age                0\n",
       "Tenure             0\n",
       "Balance            0\n",
       "NumOfProducts      0\n",
       "HasCrCard          0\n",
       "IsActiveMember     0\n",
       "EstimatedSalary    0\n",
       "Exited             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# re-check for missing values\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "xNTlfSGMRCIR"
   },
   "outputs": [],
   "source": [
    "# convert data to the correct data type\n",
    "def convert_to_type(df, cols, type_val):\n",
    "    for col in cols:\n",
    "        df[col] = df[col].astype(type_val)\n",
    "        \n",
    "convert_to_type(df, ['Surname', 'Geography', 'Gender'], str)\n",
    "convert_to_type(df, ['RowNumber', 'CustomerId', 'CreditScore', 'Age', 'Tenure', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'Exited'], 'int64')\n",
    "convert_to_type(df, ['Balance', 'EstimatedSalary'], float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dwu4h2tuRCIR",
    "outputId": "c95e9739-f778-4454-b6d7-0efd1ba67a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 14 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   RowNumber        10000 non-null  int64  \n",
      " 1   CustomerId       10000 non-null  int64  \n",
      " 2   Surname          10000 non-null  object \n",
      " 3   CreditScore      10000 non-null  int64  \n",
      " 4   Geography        10000 non-null  object \n",
      " 5   Gender           10000 non-null  object \n",
      " 6   Age              10000 non-null  int64  \n",
      " 7   Tenure           10000 non-null  int64  \n",
      " 8   Balance          10000 non-null  float64\n",
      " 9   NumOfProducts    10000 non-null  int64  \n",
      " 10  HasCrCard        10000 non-null  int64  \n",
      " 11  IsActiveMember   10000 non-null  int64  \n",
      " 12  EstimatedSalary  10000 non-null  float64\n",
      " 13  Exited           10000 non-null  int64  \n",
      "dtypes: float64(2), int64(9), object(3)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "# information about the dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GK5vhqeKRCIS"
   },
   "source": [
    "Now we don't have any missing values and we have changed the datatype in the dataset. Replacing the missing values seems like a better option than dropping the columns with missing values. Care should be taken when replacing missing values. We don't want to create bias or variance in our dataset. The data has been cleaned and so it is ready for feature engineering and machine learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OeDQwau1RCIS"
   },
   "source": [
    "<div id=\"feature_engineering\">\n",
    "    <h2>Feature engineering</h2> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BfDqktSIRCIT"
   },
   "source": [
    "#### Feature preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N7Bbkm1CRCIT"
   },
   "source": [
    "In this section, we carry out feature engineering and one-hot encoding for the categorical features. We will use one-hot encoding to transform categorical features to numerical features. To do that we have to first create dummy variable and then apply one-hot encoding for categorical features. First, we drop unimportant features like `CustomerId`, `RowNumber` and `Surname` from the dataframe before proceeding with one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CreditScore Geography  Gender  Age  Tenure    Balance  NumOfProducts  \\\n",
       "0          619    France  Female   42       2       0.00              1   \n",
       "1          608     Spain  Female   41       1   83807.86              1   \n",
       "2          502    France  Female   42       8  159660.80              3   \n",
       "3          699    France  Female   39       1       0.00              2   \n",
       "4          850     Spain  Female   43       2  125510.82              1   \n",
       "\n",
       "   HasCrCard  IsActiveMember  EstimatedSalary  Exited  \n",
       "0          1               1        101348.88       1  \n",
       "1          0               1        112542.58       0  \n",
       "2          1               0        113931.57       1  \n",
       "3          0               0         93826.63       0  \n",
       "4          1               1         79084.10       0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop unimportant features\n",
    "df = df.drop(['CustomerId', 'RowNumber', 'Surname'], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 491
    },
    "id": "AXnnBClSRCIT",
    "outputId": "32413b30-887a-4786-f187-6c32ed03ecc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train set now contains 7000 observations representing 70% of the data\n",
      "The test set now contains 3000 observations representing 30% of the data\n",
      "\n",
      "\u001b[1mShape of features and target\u001b[0m\n",
      "------------------------------\n",
      "Train features : (7000, 11)\n",
      "Train target   : (7000,)\n",
      "Test features  : (3000, 11)\n",
      "Test target    : (3000,)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Geography_Germany</th>\n",
       "      <th>Geography_Spain</th>\n",
       "      <th>Gender_Male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9716</th>\n",
       "      <td>1.658077</td>\n",
       "      <td>0.012853</td>\n",
       "      <td>-0.014739</td>\n",
       "      <td>0.635477</td>\n",
       "      <td>2.527132</td>\n",
       "      <td>0.645536</td>\n",
       "      <td>0.955284</td>\n",
       "      <td>1.480907</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>0.198643</td>\n",
       "      <td>0.584111</td>\n",
       "      <td>0.335005</td>\n",
       "      <td>0.375870</td>\n",
       "      <td>-0.895510</td>\n",
       "      <td>0.645536</td>\n",
       "      <td>0.955284</td>\n",
       "      <td>0.153167</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>-1.374648</td>\n",
       "      <td>0.774530</td>\n",
       "      <td>0.335005</td>\n",
       "      <td>1.302947</td>\n",
       "      <td>0.815811</td>\n",
       "      <td>-1.549099</td>\n",
       "      <td>-1.046809</td>\n",
       "      <td>0.817773</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7507</th>\n",
       "      <td>-0.784664</td>\n",
       "      <td>0.488901</td>\n",
       "      <td>1.384236</td>\n",
       "      <td>0.696496</td>\n",
       "      <td>-0.895510</td>\n",
       "      <td>0.645536</td>\n",
       "      <td>-1.046809</td>\n",
       "      <td>0.329403</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>2.051400</td>\n",
       "      <td>2.583513</td>\n",
       "      <td>-0.364483</td>\n",
       "      <td>-1.222967</td>\n",
       "      <td>0.815811</td>\n",
       "      <td>-1.549099</td>\n",
       "      <td>0.955284</td>\n",
       "      <td>-0.617269</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CreditScore       Age    Tenure   Balance  NumOfProducts  HasCrCard  \\\n",
       "9716     1.658077  0.012853 -0.014739  0.635477       2.527132   0.645536   \n",
       "224      0.198643  0.584111  0.335005  0.375870      -0.895510   0.645536   \n",
       "589     -1.374648  0.774530  0.335005  1.302947       0.815811  -1.549099   \n",
       "7507    -0.784664  0.488901  1.384236  0.696496      -0.895510   0.645536   \n",
       "1457     2.051400  2.583513 -0.364483 -1.222967       0.815811  -1.549099   \n",
       "\n",
       "      IsActiveMember  EstimatedSalary  Geography_Germany  Geography_Spain  \\\n",
       "9716        0.955284         1.480907                  1                0   \n",
       "224         0.955284         0.153167                  1                0   \n",
       "589        -1.046809         0.817773                  0                1   \n",
       "7507       -1.046809         0.329403                  1                0   \n",
       "1457        0.955284        -0.617269                  0                0   \n",
       "\n",
       "      Gender_Male  \n",
       "9716            1  \n",
       "224             1  \n",
       "589             0  \n",
       "7507            1  \n",
       "1457            1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# one-hot encoding of categorical features\n",
    "df_ohe = pd.get_dummies(df, drop_first=True) \n",
    "    \n",
    "# declare variables for features and target\n",
    "target = df_ohe['Exited']\n",
    "features = df_ohe.drop(['Exited'], axis=1)\n",
    "    \n",
    "# split data into training and testing \n",
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "    features, target, test_size=0.30, random_state=12345\n",
    ")\n",
    "\n",
    "# display the shape of the split dataset\n",
    "print('The train set now contains {}'.format(features_train.shape[0]) + ' observations representing 70% of the data')\n",
    "print('The test set now contains {}'.format(features_test.shape[0]) + ' observations representing 30% of the data')\n",
    "print()\n",
    "\n",
    "# numeric features in dataset\n",
    "numeric = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', \n",
    "           'HasCrCard', 'IsActiveMember', 'EstimatedSalary']\n",
    "\n",
    "# features scaling\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(features_train[numeric])\n",
    "# transform the training set and the test set using transform()\n",
    "features_train[numeric] = scaler.transform(features_train[numeric])\n",
    "features_test[numeric]  = scaler.transform(features_test[numeric])\n",
    "    \n",
    "print(\"\\033[1m\" + 'Shape of features and target' + \"\\033[0m\")\n",
    "print('-'*30)\n",
    "print('Train features :', features_train.shape)\n",
    "print('Train target   :',target_train.shape)\n",
    "print('Test features  :',features_test.shape)\n",
    "print('Test target    :',target_test.shape)\n",
    "print()\n",
    "display(features_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R-EhYnY4RCIU"
   },
   "source": [
    "**Conclusion**\n",
    "\n",
    "We encoded the categorical feature using one-hot encoding. By now, we have a lot of data from the one-hot encoding process. But when data is abundant, we have a chance of falling into the dummy feature trap. If we keep the features as they are now, it will hinder the training process. We have added 3 new features to our table from the one-hot encoding process, but their high correlation will confuse our model. To avoid this, we can safely remove any one column, since its values can be easily inferred from one of the other two columns (it has 1 where the other two columns have zeroes, and it has zeroes everywhere else). This way, we will not fall into the dummy trap. Pandas library has a function pd.get_dummies() that can be used for getting dummy variables. We split the data two ways into 70% training set, and 30% testing sets. Since the features have different scales, we standardized the numerical features of the data. The size of the new table is 7000 rows and 11 columns for the train features set, and 3000 rows and 11 columns for the test features set. Now the data is prepared and ready for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5aVQ3NOERCIU"
   },
   "source": [
    "<div id=\"class_balance\">\n",
    "    <h2>Examine the balance of classes</h2> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "JDwan5pfRCIU"
   },
   "outputs": [],
   "source": [
    "# function to calculate model evaluation metrics\n",
    "def print_model_evaluation(y_test, test_predictions):\n",
    "    print(\"\\033[1m\" + 'F1 score: ' + \"\\033[0m\", '{:.3f}'.format(f1_score(y_test, test_predictions)))\n",
    "    print(\"\\033[1m\" + 'Accuracy Score: ' + \"\\033[0m\", '{:.2%}'.format(accuracy_score(y_test, test_predictions)))\n",
    "    print(\"\\033[1m\" + 'Precision: ' + \"\\033[0m\", '{:.3f}'.format(precision_score(y_test, test_predictions)))\n",
    "    print(\"\\033[1m\" + 'Recall: ' + \"\\033[0m\", '{:.3f}'.format(recall_score(y_test, test_predictions)))\n",
    "    print(\"\\033[1m\" + 'Balanced Accuracy Score: ' + \"\\033[0m\", '{:.2%}'.format(balanced_accuracy_score(y_test, test_predictions)))\n",
    "    print(\"\\033[1m\" + 'ROC AUC Score: ' + \"\\033[0m\", '{:.2%}'.format(roc_auc_score(y_test, test_predictions)))\n",
    "    print()\n",
    "    print(\"\\033[1m\" + 'Confusion Matrix' + \"\\033[0m\")\n",
    "    print('-'*50)\n",
    "    print(confusion_matrix(y_test, test_predictions))\n",
    "    print()\n",
    "    print(\"\\033[1m\" + 'Classification report' + \"\\033[0m\")\n",
    "    print('-'*50)\n",
    "    print(classification_report(y_test, test_predictions))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first start by looking at a baseline model and see the model evaluation score which will serve as a baseline score for the different models we will be evaluating. Since we are try to look for the best model in terms of precision and recall, we optimize the F1 score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline model using a dummy classifier\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy_clf.fit(features_train, target_train)\n",
    "dummy_clf_test_predictions = dummy_clf.predict(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mF1 score: \u001b[0m 0.000\n",
      "\u001b[1mAccuracy Score: \u001b[0m 79.13%\n",
      "\u001b[1mPrecision: \u001b[0m 0.000\n",
      "\u001b[1mRecall: \u001b[0m 0.000\n",
      "\u001b[1mBalanced Accuracy Score: \u001b[0m 50.00%\n",
      "\u001b[1mROC AUC Score: \u001b[0m 50.00%\n",
      "\n",
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "--------------------------------------------------\n",
      "[[2374    0]\n",
      " [ 626    0]]\n",
      "\n",
      "\u001b[1mClassification report\u001b[0m\n",
      "--------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      1.00      0.88      2374\n",
      "           1       0.00      0.00      0.00       626\n",
      "\n",
      "    accuracy                           0.79      3000\n",
      "   macro avg       0.40      0.50      0.44      3000\n",
      "weighted avg       0.63      0.79      0.70      3000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate baseline model\n",
    "print_model_evaluation(target_test, dummy_clf_test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline model predicts the most frequent class in this case \"0\". This helps to check whether our model is actually learning something from the data. Looking at the baseline model report, we can see that the accuracy is high but the F1 score is 0.0. This is due to class imbalance. This goes to show that accuracy alone is not a good yardstick to evaluate the performance of the model. Next, we examine the balance of classes and train the model without taking into account the imbalance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "GnS83f2VRCIU"
   },
   "outputs": [],
   "source": [
    "# create the logistic regression model\n",
    "def logistic_regression(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    This is a logistic regression model function developed to train\n",
    "    the model, make prediction on train and test dataset, print\n",
    "    model accuracy for training and testing datasets\n",
    "    \"\"\"\n",
    "    model = LogisticRegression(random_state=12345, solver='liblinear')\n",
    "    model.fit(X_train, y_train) # train the model \n",
    "    train_predictions = model.predict(X_train) # make predictions on train set\n",
    "    test_predictions = model.predict(X_test) # make prediction on test set\n",
    "    print('F1 score for logistic regression model')\n",
    "    print('-'*35)\n",
    "    print('F1 score: {:.3f}'.format(f1_score(y_test, test_predictions)))\n",
    "    print()\n",
    "    print('The default F1-score of the logistic regression model is ' \"\\033[1m\" + \n",
    "          '{:.3f}.'.format(f1_score(y_test, test_predictions)) + \"\\033[0m\" +\n",
    "          ' The accuracy measured {:.3f}'.format(accuracy_score(y_train, train_predictions)) +\n",
    "          ' for training set and {:.3f}'.format(accuracy_score(y_test, test_predictions)) + ' for the testing set.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PkaA-dD5RCIU",
    "outputId": "eb302df4-9bc5-40bf-d668-451779fd3bef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for logistic regression model\n",
      "-----------------------------------\n",
      "F1 score: 0.304\n",
      "\n",
      "The default F1-score of the logistic regression model is \u001b[1m0.304.\u001b[0m The accuracy measured 0.815 for training set and 0.801 for the testing set.\n"
     ]
    }
   ],
   "source": [
    "# determine accuracy for logistic regression model\n",
    "logistic_regression(features_train, target_train, features_test, target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "id": "lKEGnlssRCIV",
    "outputId": "aa8cab35-4018-4efd-d701-be8475e0235c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.923333\n",
      "1    0.076667\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sanity check\n",
    "model = LogisticRegression(random_state=12345, solver='liblinear')\n",
    "model.fit(features_train, target_train) # train the model \n",
    "test_predictions = pd.Series(model.predict(features_test))\n",
    "class_frequency = test_predictions.value_counts(normalize=True)\n",
    "print(class_frequency)\n",
    "class_frequency.plot(kind='bar');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7UmGdSfVRCIV"
   },
   "source": [
    "In this section, we trained the model without taking into account the imbalance. We achieved an F1 score of 0.304. We assess the sanity of the model by checking how often the target feature contains the class \"1\" or \"0\". We can observe the class imbalance in the predicted testing set. Next we try to improve the quality of the model using two different approaches to fixing class imbalance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D3_-4WVnRCIV"
   },
   "source": [
    "<div id=\"improve_quality\">\n",
    "    <h2>Improve the quality of the model</h2> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ho3gKWO6RCIX"
   },
   "source": [
    "We apply two different approaches to fix the class imbalance.\n",
    "- Class weight adjustment\n",
    "- Upsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v2xQbnWYRCIX"
   },
   "source": [
    "##### Using Class Weight Adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GG0f-QIwRCIX",
    "outputId": "1fd1439f-fd77-439d-de42-0a59316d770d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score with adjusted class weight: 0.496\n"
     ]
    }
   ],
   "source": [
    "# class weight adjustment\n",
    "model = LogisticRegression(random_state=12345, class_weight='balanced', solver='liblinear')\n",
    "model.fit(features_train, target_train)\n",
    "test_predictions = model.predict(features_test) \n",
    "print('F1 score with adjusted class weight: {:.3f}'.format(f1_score(target_test, test_predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "id": "NjwBniXkRCIX",
    "outputId": "0effa872-ca44-4838-93f0-ea5535cef782"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.617667\n",
      "1    0.382333\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sanity check after class imbalance\n",
    "test_predictions = pd.Series(model.predict(features_test))\n",
    "class_frequency = test_predictions.value_counts(normalize=True)\n",
    "print(class_frequency)\n",
    "class_frequency.plot(kind='bar');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "paH09BVdRCIY"
   },
   "source": [
    "Here, we made the rare classes weigh more by specifying `class_weight='balanced'`. Notice how the F1 score increased to $\\approx$ 0.496 for a balanced class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zhm377iHRCIY"
   },
   "source": [
    "##### By Upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "2n6byWmyRCIY"
   },
   "outputs": [],
   "source": [
    "# function to perform upsampling \n",
    "def upsample(features, target, repeat):\n",
    "    features_zeros = features[target == 0]\n",
    "    features_ones = features[target == 1]\n",
    "    target_zeros = target[target == 0]\n",
    "    target_ones = target[target == 1]\n",
    "\n",
    "    features_upsampled = pd.concat([features_zeros] + [features_ones] * repeat)\n",
    "    target_upsampled = pd.concat([target_zeros] + [target_ones] * repeat)\n",
    "\n",
    "    features_upsampled, target_upsampled = shuffle(\n",
    "        features_upsampled, target_upsampled, random_state=12345\n",
    "    )\n",
    "    return features_upsampled, target_upsampled\n",
    "\n",
    "# new training set created\n",
    "features_upsampled, target_upsampled = upsample(\n",
    "    features_train, target_train, 5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IA1DQ65hRCIY",
    "outputId": "22c425ea-0cdc-480d-968f-5b4945dc134c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score after upsampling: 0.487\n"
     ]
    }
   ],
   "source": [
    "# F1 score after upsampling \n",
    "model = LogisticRegression(random_state=12345, solver='liblinear')\n",
    "model.fit(features_upsampled, target_upsampled)\n",
    "test_predictions = model.predict(features_test) \n",
    "print('F1 score after upsampling: {:.3f}'.format(f1_score(target_test, test_predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5dquZJ5iRCIY"
   },
   "source": [
    "First, we split the training sample into negative and positive observations, we duplicated the positive observations and combine them with the negative class observation. Then we shuffled the data using `shuffle()` function, and trained our *LogisticRegression* model with the new data. We calculated the F1 score to be $\\approx$ 0.487. This is an improvement from the default F1 score of 0.304 calculated initially. This shows an improvement in the F1 score when the class is balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m-Ecx_UDRCIZ"
   },
   "source": [
    "<div id=\"investigate_models\">\n",
    "    <h2>Investigate different models quality</h2> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "O3xUy1fewlqa"
   },
   "outputs": [],
   "source": [
    "# function to plot ROC curve\n",
    "def plot_roc(y_test, preds, ax=None, label='model'):\n",
    "    with plt.style.context('seaborn-whitegrid'):\n",
    "        \"\"\"\n",
    "        This function plots the ROC curve\n",
    "        \"\"\"\n",
    "        if not ax: fig, ax = plt.subplots(1, 1)\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, preds)\n",
    "        ax.plot([0, 1], [0, 1],'r--')\n",
    "        ax.plot(fpr, tpr, lw=2, label=label)\n",
    "        ax.legend(loc='lower right')\n",
    "        ax.set_title(\n",
    "            'ROC curve\\n'\n",
    "            f\"\"\" AP: {average_precision_score(\n",
    "                y_test, preds, pos_label=1\n",
    "            ):.2} | \"\"\"\n",
    "            f'AUC: {auc(fpr, tpr):.2}'\n",
    "        )\n",
    "        ax.set_xlabel('False Positive Rate (FPR)')\n",
    "        ax.set_ylabel('True Positive Rate (TPR)')\n",
    "        ax.annotate(f'AUC: {auc(fpr, tpr):.2}', xy=(.43, .025))\n",
    "        ax.legend()\n",
    "        ax.grid()\n",
    "        return ax\n",
    "    \n",
    "# function to plot the precision-recall curve\n",
    "def plot_pr(y_test, preds, ax=None, label='model'):\n",
    "    with plt.style.context('seaborn-whitegrid'):\n",
    "        \"\"\"\n",
    "        This function is used to the precision-recall curve \n",
    "        \"\"\"\n",
    "        precision, recall, thresholds = precision_recall_curve(y_test, preds)\n",
    "        if not ax: fig, ax = plt.subplots()\n",
    "        ax.plot([0, 1], [1, 0],'r--')    \n",
    "        ax.plot(recall, precision, lw=2, label=label)\n",
    "        ax.legend()\n",
    "        ax.set_title(\n",
    "            'Precision-recall curve\\n'\n",
    "            f\"\"\" AP: {average_precision_score(\n",
    "                y_test, preds, pos_label=1\n",
    "            ):.2} | \"\"\"\n",
    "            f'AUC: {auc(recall, precision):.2}'\n",
    "        )\n",
    "        ax.set_xlabel('Recall')\n",
    "        ax.set_ylabel('Precision')\n",
    "        ax.set_xlim(-0.05, 1.05)\n",
    "        ax.set_ylim(-0.05, 1.05)\n",
    "        ax.legend()\n",
    "        ax.grid()\n",
    "        return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GoilUtX_RCIZ"
   },
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rvAFUoAekT2G"
   },
   "source": [
    "We are going to try and tune the hyperparameters of the following classification algorithms. A small grid searching example is given and used as a starting point for the investigation. To speed up running this section, it is best if the code is run in google colab using google's GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wAuVPvo2RCIZ"
   },
   "source": [
    "##### Decision Tree Classifier\n",
    "\n",
    "For the decision tree classifier, we iterate over different values and compare the quality of the model by tuning the `max_depth` hyperparameter. In GridSearchCV, we pass scoring='f1' to tune the target metric which is the F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wuf0kRw8jW98",
    "outputId": "3badc9ee-8135-4081-bad0-5aa40de8fcee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The parameters combination that would give best F1 score is : \n",
      "{'criterion': 'entropy', 'max_depth': 8, 'min_samples_leaf': 2, 'min_samples_split': 16}\n",
      "The best accuracy achieved after parameter tuning via grid search is: 0.555\n",
      "The accuracy of the model against the training data is: 0.632\n",
      "F1 score:  0.5411298315163529\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter optimization for Decision tree classifier\n",
    "parameters = {\n",
    "    \"criterion\" : [\"gini\", \"entropy\"],\n",
    "    \"max_depth\" : [2, 4, 8, 16],\n",
    "    \"min_samples_split\" : [2, 4, 8, 16],\n",
    "    \"min_samples_leaf\" : [2, 4, 6]\n",
    "    }\n",
    "classifier = DecisionTreeClassifier()\n",
    "grid = GridSearchCV(classifier, parameters, scoring='f1', cv=5)\n",
    "grid.fit(features_train, target_train) \n",
    "y_pred = grid.predict(features_test)\n",
    "print('The parameters combination that would give best F1 score is : ')\n",
    "print(grid.best_params_)\n",
    "print('The best accuracy achieved after parameter tuning via grid search is: {:.3f}'.format(grid.best_score_))\n",
    "print('The accuracy of the model against the training data is: {:.3f}'.format(grid.score(features_train, target_train)))\n",
    "print('F1 score: ', f1_score(target_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "0h8Q_F1BRCIZ"
   },
   "outputs": [],
   "source": [
    "# create the decision tree classifier\n",
    "def decision_tree_classifier(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    This is a decision tree classifier function developed to train  \n",
    "    the model, make prediction on train and testing dataset, print\n",
    "    the F1 score and model accuracy for training and testing datasets\n",
    "    \"\"\"\n",
    "    # define lists to collect scores\n",
    "    train_scores = []\n",
    "    test_scores = []\n",
    "    f1_scores = []\n",
    "    model = DecisionTreeClassifier(**grid.best_params_) \n",
    "    model.fit(X_train, y_train) # train the model\n",
    "    # make predictions on train set\n",
    "    train_predictions = model.predict(X_train)\n",
    "    train_predictions_acc = accuracy_score(y_train, train_predictions)\n",
    "    train_scores.append(train_predictions_acc)\n",
    "    # make predictions on testing set\n",
    "    dt_test_predictions = model.predict(X_test) \n",
    "    dt_test_predictions_acc = accuracy_score(y_test, dt_test_predictions)\n",
    "    test_scores.append(dt_test_predictions_acc)\n",
    "    f1_score_ = f1_score(y_test, dt_test_predictions)\n",
    "    f1_scores.append(f1_score_)\n",
    "    scores = list(zip(f1_scores, train_scores, test_scores))\n",
    "    print('The decision tree classifier had the best ' \"\\033[1m\" + 'F1 score of {:.3f}'.format(max(scores, key = lambda x: x[0])[0]) + \"\\033[0m\" +   \n",
    "          ' and accuracy of ' \"\\033[1m\" '{:.2%}'.format(max(scores, key = lambda x: x[0])[1]) + ' for the training set' + \"\\033[0m\" + \n",
    "          ' and ' + \"\\033[1m\" '{:.2%}'.format(max(scores, key = lambda x: x[0])[2]) + ' for the testing set' + \"\\033[0m\")\n",
    "    print()\n",
    "    # evaluate decision tree classifier metric\n",
    "    print_model_evaluation(target_test, dt_test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kg6hr7ZDRCIZ",
    "outputId": "7bdaf560-7197-4238-c936-3a58aea0f66c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The decision tree classifier had the best \u001b[1mF1 score of 0.537\u001b[0m and accuracy of \u001b[1m87.90% for the training set\u001b[0m and \u001b[1m84.53% for the testing set\u001b[0m\n",
      "\n",
      "\u001b[1mF1 score: \u001b[0m 0.537\n",
      "\u001b[1mAccuracy Score: \u001b[0m 84.53%\n",
      "\u001b[1mPrecision: \u001b[0m 0.715\n",
      "\u001b[1mRecall: \u001b[0m 0.430\n",
      "\u001b[1mBalanced Accuracy Score: \u001b[0m 69.23%\n",
      "\u001b[1mROC AUC Score: \u001b[0m 69.23%\n",
      "\n",
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "--------------------------------------------------\n",
      "[[2267  107]\n",
      " [ 357  269]]\n",
      "\n",
      "\u001b[1mClassification report\u001b[0m\n",
      "--------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.95      0.91      2374\n",
      "           1       0.72      0.43      0.54       626\n",
      "\n",
      "    accuracy                           0.85      3000\n",
      "   macro avg       0.79      0.69      0.72      3000\n",
      "weighted avg       0.83      0.85      0.83      3000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# determine F1 score for decision tree classifier\n",
    "decision_tree_classifier(features_train, target_train, features_test, target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46AIjXpcRCIa"
   },
   "source": [
    "We used GridSearchCV to carry out hyperparameter optimization on the `max_depth`, `criterion`, `min_samples_split`, and `min_samples_leaf` parameters. We looking for the best parameter setting for our decision tree classifier. We note that shallow decision trees (e.g. few depth) generally do not overfit but have poor performance (high bias, low variance), and deep trees (e.g. high depth) generally do overfit and have good performance (low bias, high variance). Our desirable tree depth is one that is not so shallow that it has low performance and not so deep that it overfits the training dataset. We need to have a balance between bias and variance - bias variance tradeoff. At `max_depth` of 8, we have an F1 score of 0.54, an accuracy of 87.90% for the training set, and 84.53% for the testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KKDdBsG2RCIa"
   },
   "source": [
    "##### Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZPrS67ElSsP5",
    "outputId": "b738f2ff-286d-4f7e-94d9-31d7963855a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The parameters combination that would give best F1 score is : \n",
      "{'C': 10, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "The best accuracy achieved after parameter tuning via grid search is: 0.325\n",
      "The accuracy of the model against the training data is: 0.329\n",
      "F1 score:  0.3057176196032672\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter optimization\n",
    "# define parameters\n",
    "grid = {\n",
    "    \"solver\" : ['newton-cg', 'lbfgs', 'liblinear'],\n",
    "    \"penalty\" : ['l2'],\n",
    "    \"C\" : [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "}\n",
    "# define grid search\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# define the model\n",
    "regressor = LogisticRegression()\n",
    "grid_search = GridSearchCV(estimator = regressor, param_grid = grid, \n",
    "                           n_jobs=-1, cv=cv, scoring='f1', error_score=0)\n",
    "grid_search.fit(features_train, target_train) \n",
    "y_pred = grid_search.predict(features_test)\n",
    "print('The parameters combination that would give best F1 score is : ')\n",
    "print(grid_search.best_params_)\n",
    "print('The best accuracy achieved after parameter tuning via grid search is: {:.3f}'.format(grid_search.best_score_))\n",
    "print('The accuracy of the model against the training data is: {:.3f}'.format(grid_search.score(features_train, target_train)))\n",
    "print('F1 score: ', f1_score(target_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "07r70Q27RCIa"
   },
   "outputs": [],
   "source": [
    "# create the logistic regression model\n",
    "def logistic_regression(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    This is a logistic regression model function developed to train\n",
    "    the model, make prediction on train and testing dataset, \n",
    "    and print F1 score and evaluation metrics for testing datasets\n",
    "    \"\"\"\n",
    "    # define lists to collect scores\n",
    "    train_scores = []\n",
    "    test_scores = []\n",
    "    f1_scores = []\n",
    "    model = LogisticRegression(**grid_search.best_params_)\n",
    "    model.fit(X_train, y_train) # train the model \n",
    "    # make predictions on train set\n",
    "    train_predictions = model.predict(X_train)\n",
    "    train_predictions_acc = accuracy_score(y_train, train_predictions)\n",
    "    train_scores.append(train_predictions_acc)\n",
    "    # make predictions on testing set\n",
    "    lr_test_predictions = model.predict(X_test) \n",
    "    lr_test_predictions_acc = accuracy_score(y_test, lr_test_predictions)\n",
    "    test_scores.append(lr_test_predictions_acc)\n",
    "    f1_score_ = f1_score(y_test, lr_test_predictions)\n",
    "    f1_scores.append(f1_score_)\n",
    "    scores = list(zip(f1_scores, train_scores, test_scores))\n",
    "    print('The logistic regression classifier had the best ' \"\\033[1m\" + \n",
    "          'F1 score of {:.3f} '.format(max(scores, key = lambda x: x[0])[0]) + \"\\033[0m\" + \n",
    "          'using' \"\\033[1m\" + ' C parameter of {},'.format(grid_search.best_params_['C']) + \"\\033[0m\" +\n",
    "          \"\\033[1m\" + ' {} as logistic regression solver'.format(grid_search.best_params_['solver']) + \"\\033[0m\" +\n",
    "          ' leading to an accuracy of ' \"\\033[1m\" + '{:.2%}'.format(max(scores, key = lambda x: x[0])[1]) + ' for the training set ' + \"\\033[0m\" + \n",
    "          'and ' + \"\\033[1m\" '{:.2%}'.format(max(scores, key = lambda x: x[0])[2]) + ' for the testing set' + \"\\033[0m\")\n",
    "    print()\n",
    "    # evaluate logistic regression classifier metric\n",
    "    print_model_evaluation(target_test, lr_test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KtHlakjQRCIa",
    "outputId": "cd1f9a40-147b-4886-cea3-326552122a16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The logistic regression classifier had the best \u001b[1mF1 score of 0.306 \u001b[0musing\u001b[1m C parameter of 10,\u001b[0m\u001b[1m newton-cg as logistic regression solver\u001b[0m leading to an accuracy of \u001b[1m81.49% for the training set \u001b[0mand \u001b[1m80.17% for the testing set\u001b[0m\n",
      "\n",
      "\u001b[1mF1 score: \u001b[0m 0.306\n",
      "\u001b[1mAccuracy Score: \u001b[0m 80.17%\n",
      "\u001b[1mPrecision: \u001b[0m 0.567\n",
      "\u001b[1mRecall: \u001b[0m 0.209\n",
      "\u001b[1mBalanced Accuracy Score: \u001b[0m 58.36%\n",
      "\u001b[1mROC AUC Score: \u001b[0m 58.36%\n",
      "\n",
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "--------------------------------------------------\n",
      "[[2274  100]\n",
      " [ 495  131]]\n",
      "\n",
      "\u001b[1mClassification report\u001b[0m\n",
      "--------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.96      0.88      2374\n",
      "           1       0.57      0.21      0.31       626\n",
      "\n",
      "    accuracy                           0.80      3000\n",
      "   macro avg       0.69      0.58      0.60      3000\n",
      "weighted avg       0.77      0.80      0.76      3000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# determine F1 score for logistic regression model\n",
    "logistic_regression(features_train, target_train, features_test, target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GVtwopghRCIa"
   },
   "source": [
    "We tuned the \"C\" parameter for the logistic regression model. Although the model training is fast, the F1 score is lower at 0.306. The logistic regression model gave an accuracy of 81.49% for the training set, and 80.17% for the testing sets when using a \"C\" parameter of 10. We can see here that neither the training nor the testing score is high enough. This is because the model is not complex enough hence underfitting occurs. Let's see how other model behave before deciding on the model to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qcPWkp95RCIb"
   },
   "source": [
    "##### AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ON56CqnBRVhP"
   },
   "source": [
    "AdaBoost is challenging to configure since the algorithm has many key hyperparameters that influence the behavior of the model on the training data, and the hyperparameters interact with each other. In such as senario, it is best practice to use a grid search process to discover a configuration of the model hyperparameters that works for the given problem. In this case, we would grid search two key parameters - the number of trees used in the ensemble and the learning rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "sehg30tlSlQ4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The parameters combination that would give best F1 score is : \n",
      "{'learning_rate': 1.0, 'n_estimators': 50}\n",
      "The best accuracy achieved after parameter tuning via grid search is: 0.568\n",
      "The accuracy of the model against the training data is: 0.585\n",
      "F1 score:  0.5632973503434741\n"
     ]
    }
   ],
   "source": [
    "# GridSearchCV Optimization for Adaboost\n",
    "# define the hyperparameter to tune\n",
    "grid = {\n",
    "    \"n_estimators\" : [10, 50, 100, 500],\n",
    "    \"learning_rate\" : [0.0001, 0.001, 0.01, 0.1, 1.0]\n",
    "}\n",
    "# define grid search\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# define the model\n",
    "model = AdaBoostClassifier()\n",
    "# define the grid search procedure\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, \n",
    "                           n_jobs=-1, cv=cv, scoring='f1')\n",
    "# execute the grid search\n",
    "grid_search.fit(features_train, target_train) \n",
    "y_pred = grid_search.predict(features_test)\n",
    "print('The parameters combination that would give best F1 score is : ')\n",
    "print(grid_search.best_params_)\n",
    "print('The best accuracy achieved after parameter tuning via grid search is: {:.3f}'.format(grid_search.best_score_))\n",
    "print('The accuracy of the model against the training data is: {:.3f}'.format(grid_search.score(features_train, target_train)))\n",
    "print('F1 score: ', f1_score(target_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fUzCgkuvWlsd"
   },
   "source": [
    "We defined the grid search using k-fold cross-validation, and carried out grisearchcv optimization for the adaboost classifier. We looked for the configuration with the best score. We can see that the parameter configuration with 50 trees and a learning rate of 1.0 gave the best F1 score. We can use this figures as a guide to train and test the model or simply run a for loop through a set parameter values and learning rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "3D5msVSpRCIb"
   },
   "outputs": [],
   "source": [
    "# create adaboost classifier\n",
    "def adaboost_classifier(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    This is an Adaboost classifier function developed to train\n",
    "    the model, make prediction on train and testing dataset, \n",
    "    and print F1 score and evaluation metrics for testing datasets\n",
    "    \"\"\"\n",
    "    # define lists to collect scores\n",
    "    train_scores = []\n",
    "    test_scores = []\n",
    "    f1_scores = []\n",
    "    model = AdaBoostClassifier(**grid_search.best_params_)\n",
    "    model.fit(X_train, y_train) # train the model \n",
    "    # make predictions on train set\n",
    "    train_predictions = model.predict(X_train)\n",
    "    train_predictions_acc = accuracy_score(y_train, train_predictions)\n",
    "    train_scores.append(train_predictions_acc)\n",
    "    # make predictions on testing set\n",
    "    ab_test_predictions = model.predict(X_test) \n",
    "    ab_test_predictions_acc = accuracy_score(y_test, ab_test_predictions)\n",
    "    test_scores.append(ab_test_predictions_acc)\n",
    "    f1_score_ = f1_score(y_test, ab_test_predictions)\n",
    "    f1_scores.append(f1_score_)\n",
    "    scores = list(zip(f1_scores, train_scores, test_scores))\n",
    "    print('The adaboost classifier had the best ' \"\\033[1m\" + \n",
    "          'F1 score of {:.3f} '.format(max(scores, key = lambda x: x[0])[0]) + \"\\033[0m\" + \n",
    "          'using' \"\\033[1m\" + ' n_estimate value of {},'.format(grid_search.best_params_['n_estimators']) + \"\\033[0m\" +\n",
    "          \"\\033[1m\" + ' learning rate of {}'.format(grid_search.best_params_['learning_rate']) + \"\\033[0m\" +\n",
    "          ' giving an accuracy of ' \"\\033[1m\" + '{:.2%}'.format(max(scores, key = lambda x: x[0])[1]) + ' for the training set ' + \"\\033[0m\" + \n",
    "          'and ' + \"\\033[1m\" '{:.2%}'.format(max(scores, key = lambda x: x[0])[2]) + ' for the testing set' + \"\\033[0m\")\n",
    "    print()\n",
    "    # evaluate adaboost classifier metric\n",
    "    print_model_evaluation(target_test, ab_test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "1MOOf_z0RCIb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The adaboost classifier had the best \u001b[1mF1 score of 0.563 \u001b[0musing\u001b[1m n_estimate value of 50,\u001b[0m\u001b[1m learning rate of 1.0\u001b[0m giving an accuracy of \u001b[1m86.23% for the training set \u001b[0mand \u001b[1m85.17% for the testing set\u001b[0m\n",
      "\n",
      "\u001b[1mF1 score: \u001b[0m 0.563\n",
      "\u001b[1mAccuracy Score: \u001b[0m 85.17%\n",
      "\u001b[1mPrecision: \u001b[0m 0.730\n",
      "\u001b[1mRecall: \u001b[0m 0.458\n",
      "\u001b[1mBalanced Accuracy Score: \u001b[0m 70.69%\n",
      "\u001b[1mROC AUC Score: \u001b[0m 70.69%\n",
      "\n",
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "--------------------------------------------------\n",
      "[[2268  106]\n",
      " [ 339  287]]\n",
      "\n",
      "\u001b[1mClassification report\u001b[0m\n",
      "--------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.96      0.91      2374\n",
      "           1       0.73      0.46      0.56       626\n",
      "\n",
      "    accuracy                           0.85      3000\n",
      "   macro avg       0.80      0.71      0.74      3000\n",
      "weighted avg       0.84      0.85      0.84      3000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# determine F1 score for adaboost classifier\n",
    "adaboost_classifier(features_train, target_train, features_test, target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VO5RwXJjYhB8"
   },
   "source": [
    "We tuned the `n_estimators` and the learning rate for the AdaBoost classifier resulting in an F1 score of 0.56, accuracy of 86.23% for the training set, and 85.17% for the testing sets when using 50 trees and a learning rate of 1.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wsqTF4TcRCIb"
   },
   "source": [
    "##### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "-TrPm9xmJNpZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The parameters combination that would give best F1 score is : \n",
      "{'criterion': 'gini', 'max_depth': 16, 'min_samples_leaf': 6, 'min_samples_split': 8, 'n_estimators': 10}\n",
      "The best accuracy achieved after parameter tuning via grid search is: 0.580\n",
      "The accuracy of the model against the training data is: 0.677\n",
      "F1 score:  0.5595116988809766\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter optimization for Random Forest classifier\n",
    "# define the hyperparameter to tune\n",
    "parameters = {\n",
    "    \"criterion\" : [\"gini\", \"entropy\"],\n",
    "    \"n_estimators\" : [10, 50, 100, 200, 500],\n",
    "    \"max_depth\" : [None, 2, 4, 8, 10, 12, 16],\n",
    "    \"min_samples_split\" : [2, 4, 8, 16],\n",
    "    \"min_samples_leaf\" : [2, 4, 6]\n",
    "    }\n",
    "# define the model with default hyperparameters\n",
    "classifier = RandomForestClassifier()\n",
    "# define the grid search procedure\n",
    "grid_rf = GridSearchCV(estimator=classifier, param_grid=parameters, \n",
    "                    cv=5, scoring='f1')\n",
    "# execute the grid search\n",
    "grid_rf.fit(features_train, target_train) \n",
    "y_pred = grid_rf.predict(features_test)\n",
    "print('The parameters combination that would give best F1 score is : ')\n",
    "print(grid_rf.best_params_)\n",
    "print('The best accuracy achieved after parameter tuning via grid search is: {:.3f}'.format(grid_rf.best_score_))\n",
    "print('The accuracy of the model against the training data is: {:.3f}'.format(grid_rf.score(features_train, target_train)))\n",
    "print('F1 score: ', f1_score(target_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "zb01edKyRCIb"
   },
   "outputs": [],
   "source": [
    "# create the random forest classifier model\n",
    "def random_forest_classifier(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    This is a random forest classifier function developed to train\n",
    "    the model, make prediction on train and testing dataset, \n",
    "    and print F1 score and evaluation metrics for testing datasets\n",
    "    \"\"\"\n",
    "    # define lists to collect scores\n",
    "    train_scores = []\n",
    "    test_scores = []\n",
    "    f1_scores = []\n",
    "    model = RandomForestClassifier(**grid_rf.best_params_)\n",
    "    model.fit(X_train, y_train) # train the model \n",
    "    # make predictions on train set\n",
    "    train_predictions = model.predict(X_train)\n",
    "    train_predictions_acc = accuracy_score(y_train, train_predictions)\n",
    "    train_scores.append(train_predictions_acc)\n",
    "    # make predictions on testing set\n",
    "    rf_test_predictions = model.predict(X_test) \n",
    "    rf_test_predictions_acc = accuracy_score(y_test, rf_test_predictions)\n",
    "    test_scores.append(rf_test_predictions_acc)\n",
    "    f1_score_ = f1_score(y_test, rf_test_predictions)\n",
    "    f1_scores.append(f1_score_)\n",
    "    scores = list(zip(f1_scores, train_scores, test_scores))\n",
    "    print('The random forest classifier had the best ' \"\\033[1m\" + \n",
    "          'F1 score of {:.3f} '.format(max(scores, key = lambda x: x[0])[0]) + \"\\033[0m\" + \n",
    "          'using' \"\\033[1m\" + ' n_estimate value of {},'.format(grid_rf.best_params_['n_estimators']) + \"\\033[0m\" +\n",
    "          \"\\033[1m\" + ' maximum tree depth of {}'.format(grid_rf.best_params_['max_depth']) + \"\\033[0m\" +\n",
    "          ' giving an accuracy of ' \"\\033[1m\" + '{:.2%}'.format(max(scores, key = lambda x: x[0])[1]) + ' for the training set ' + \"\\033[0m\" + \n",
    "          'and ' + \"\\033[1m\" '{:.2%}'.format(max(scores, key = lambda x: x[0])[2]) + ' for the test set' + \"\\033[0m\")\n",
    "    print()\n",
    "    # evaluate random forest classifier metric\n",
    "    print_model_evaluation(target_test, rf_test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "fGwmZiYQRCIc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The random forest classifier had the best \u001b[1mF1 score of 0.544 \u001b[0musing\u001b[1m n_estimate value of 10,\u001b[0m\u001b[1m maximum tree depth of 16\u001b[0m giving an accuracy of \u001b[1m89.76% for the training set \u001b[0mand \u001b[1m85.23% for the test set\u001b[0m\n",
      "\n",
      "\u001b[1mF1 score: \u001b[0m 0.544\n",
      "\u001b[1mAccuracy Score: \u001b[0m 85.23%\n",
      "\u001b[1mPrecision: \u001b[0m 0.765\n",
      "\u001b[1mRecall: \u001b[0m 0.422\n",
      "\u001b[1mBalanced Accuracy Score: \u001b[0m 69.38%\n",
      "\u001b[1mROC AUC Score: \u001b[0m 69.38%\n",
      "\n",
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "--------------------------------------------------\n",
      "[[2293   81]\n",
      " [ 362  264]]\n",
      "\n",
      "\u001b[1mClassification report\u001b[0m\n",
      "--------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.97      0.91      2374\n",
      "           1       0.77      0.42      0.54       626\n",
      "\n",
      "    accuracy                           0.85      3000\n",
      "   macro avg       0.81      0.69      0.73      3000\n",
      "weighted avg       0.84      0.85      0.84      3000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# determine accuracy for random forest classifier\n",
    "random_forest_classifier(features_train, target_train, features_test, target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JSiH7VjVZQPj"
   },
   "source": [
    "We tuned the `max_depth` and `n_estimators` for the Random Forest classifier resulting in an F1 score of 0.54, accuracy of 89.76% for the training set, and 85.23% for the testing sets when using 16 trees and n_estimate value of 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the CatBoost classifier model\n",
    "def catboost_classifier(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    This is a catboost classifier function developed to train\n",
    "    the model, make prediction on train and testing dataset, \n",
    "    and print F1 score and evaluation metrics for testing datasets\n",
    "    \"\"\"\n",
    "    # define model\n",
    "    model = CatBoostClassifier(verbose=0, random_state=12345)\n",
    "    model.fit(X_train, y_train) # train the model \n",
    "    # make predictions on train set\n",
    "    train_predictions = model.predict(X_train)\n",
    "    train_predictions_acc = accuracy_score(y_train, train_predictions)\n",
    "    # make predictions on testing set\n",
    "    cb_test_predictions = model.predict(X_test) \n",
    "    cb_test_predictions_acc = accuracy_score(y_test, cb_test_predictions)\n",
    "    f1_score_ = f1_score(y_test, cb_test_predictions)\n",
    "    print('The model has an ' \"\\033[1m\" 'F1 score of {:.3f},'.format(f1_score_) + \"\\033[0m\" +\n",
    "          ' accuracy of ' \"\\033[1m\" + '{:.2%}'.format(train_predictions_acc) + ' for the training set ' + \"\\033[0m\" + \n",
    "          'and ' + \"\\033[1m\" '{:.2%}'.format(cb_test_predictions_acc) + ' for the testing set' + \"\\033[0m\")\n",
    "    print()\n",
    "    # evaluate catboost classifier metric\n",
    "    print_model_evaluation(target_test, cb_test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has an \u001b[1mF1 score of 0.587,\u001b[0m accuracy of \u001b[1m91.26% for the training set \u001b[0mand \u001b[1m86.07% for the testing set\u001b[0m\n",
      "\n",
      "\u001b[1mF1 score: \u001b[0m 0.587\n",
      "\u001b[1mAccuracy Score: \u001b[0m 86.07%\n",
      "\u001b[1mPrecision: \u001b[0m 0.769\n",
      "\u001b[1mRecall: \u001b[0m 0.474\n",
      "\u001b[1mBalanced Accuracy Score: \u001b[0m 71.85%\n",
      "\u001b[1mROC AUC Score: \u001b[0m 71.85%\n",
      "\n",
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "--------------------------------------------------\n",
      "[[2285   89]\n",
      " [ 329  297]]\n",
      "\n",
      "\u001b[1mClassification report\u001b[0m\n",
      "--------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.96      0.92      2374\n",
      "           1       0.77      0.47      0.59       626\n",
      "\n",
      "    accuracy                           0.86      3000\n",
      "   macro avg       0.82      0.72      0.75      3000\n",
      "weighted avg       0.85      0.86      0.85      3000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# determine F1 score for CatBoost algorithm\n",
    "catboost_classifier(features_train, target_train, features_test, target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CatBoost classifier gave the best F1 score of 0.59."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Summary of results\n",
    "\n",
    "|Model (Classifier) | F1 Score | Accuracy | AUC-ROC |\n",
    "|:-------|-------|-------|-------|\n",
    "|Decision Tree  | 0.54 | 84.53% | 69.23% |\n",
    "|Logistic Regression  | 0.31 | 80.17% | 58.36% |\n",
    "|Adaboost | 0.56 | 85.17% | 70.69% |\n",
    "|Random Forest | 0.54| 85.23% | 69.38% |\n",
    "|Catboost | 0.59| 86.07% | 71.85% |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c9Lev3-HRCIc"
   },
   "source": [
    "**Conclusion**\n",
    "\n",
    "From the investigation of different model quality, we can see that the CatBoost classifier gives the best result for F1 score of 0.59, accuracy of 86.07% and AUC-ROC value of 71.85% of the five different models investigated. The logistic regression model had the lowest values for F1 score of 0.31, accuracy of 80.17%, and AUC-ROC of 58.36%. The Catboost model is the best model based on the F1 score when predicting whether a customer will leave the bank soon.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_o_TEmBzRCIc"
   },
   "source": [
    "<div id=\"check_quality\">\n",
    "    <h2>Check model quality</h2> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZHxqO8hf60t4"
   },
   "source": [
    "#### Model testing\n",
    "\n",
    "The result of the previous section suggested that the CatBoost classifier was perhaps the most accurate model. Using the CatBoost classifier as our final model, we can make predictions using the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the CatBoost classifier model\n",
    "def catboost_classifier(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    This is a catboost classifier function developed to train\n",
    "    the model, make prediction on train and test dataset, print\n",
    "    model accuracy for training and test datasets and visualize\n",
    "    model accuracy scores on train and test sets\n",
    "    \"\"\"\n",
    "    # define model\n",
    "    model = CatBoostClassifier(verbose=0, random_state=12345)\n",
    "    model.fit(X_train, y_train) # train the model \n",
    "    # make predictions on train set\n",
    "    train_predictions = model.predict(X_train)\n",
    "    train_predictions_acc = accuracy_score(y_train, train_predictions)\n",
    "    # make predictions on test set\n",
    "    test_predictions = model.predict(X_test)\n",
    "    test_predictions_acc = accuracy_score(y_test, test_predictions)\n",
    "    f1_score_ = f1_score(y_test, test_predictions)\n",
    "    print('The model has an ' \"\\033[1m\" 'F1 score of {:.3f},'.format(f1_score_) + \"\\033[0m\" +\n",
    "          ' accuracy of ' \"\\033[1m\" + '{:.2%}'.format(train_predictions_acc) + ' for the training set ' + \"\\033[0m\" + \n",
    "          'and ' + \"\\033[1m\" '{:.2%}'.format(test_predictions_acc) + ' for the test set' + \"\\033[0m\")\n",
    "    # plot of ROC and Precision-Recall curve\n",
    "    _, axs = plt.subplots(1, 2,figsize=(10,5))\n",
    "    axs = axs.ravel()\n",
    "    plot_pr(y_test, test_predictions, ax=axs[0], label=\"CatBoostClassifier\")\n",
    "    plot_roc(y_test, test_predictions, ax=axs[1], label=\"CatBoostClassifier\")\n",
    "    plt.title('Plot of ROC and Precision-Recall curve')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has an \u001b[1mF1 score of 0.587,\u001b[0m accuracy of \u001b[1m91.26% for the training set \u001b[0mand \u001b[1m86.07% for the test set\u001b[0m\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# determine accuracy for CatBoost algorithm\n",
    "catboost_classifier(features_train, target_train, features_test, target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HnWd1CXl9qgP"
   },
   "source": [
    "Using the CatBoost model, we obtained an **F1-score of 0.59**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5u6Q8LBIRCId"
   },
   "source": [
    "<div id=\"overall_conclusion\">\n",
    "    <h2>Overall conclusion</h2> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first section, we downloaded and prepared the data. From the information about the dataset, we have 10000 rows and 14 features. Looking at the dataset, we observed that about 9% of the data is missing in the Tenure column, and the data is missing at random (MAR). We replaced the missing values, corrected the datatype, and encoded the categorical feature using one-hot encoding. We splitted the data into 70% training set, and 30% testing sets. Since the features have different scales, we standardized the numerical features of the data. The size of the new table is 7000 rows and 11 columns for the train features set, and 3000 rows and 11 columns for the test features set. \n",
    "\n",
    "We trained the model without taking into account the imbalance. We achieved a F1 score of 0.304. We assess the sanity of the model by checking how often the target feature contains the class \"1\" or \"0\". We observed the class imbalance in the dataset. Next we try to improve the quality of the model using two different approaches to fixing class imbalance. First, we split the training sample into negative and positive observations, we duplicated the positive observations and combine them with the negative class observation. Then we shuffled the data using shuffle() function, and trained our LogisticRegression model with the new data. We calculated the F1 score to be 0.487.\n",
    "\n",
    "We investigated several models and tuned different hyperparameters for those model using GridSearchCV and RepeatedStratifiedKFold. From the investigation of different model quality, we observed that the CatBoost classifier gave an accuracy of 91.26% for the training data, and 86.07% for the testing data which is the best result of the five different models investigated. The logistic regression model gave the lowest accuracy prediction of the five models with an accuracy of 80.17% for the testing sets. We proceed to use the CatBoost classifier to perform the final test prediction on the unseen test data. Using the CatBoost model, we obtained an F1-score of 0.59. \n",
    "\n",
    "At the end of this project, we were able to develop a model that can predict whether a customer will leave the bank soon with an accuracy of 86% and an F1-score of **0.59**. The CatBoost model really helped the business predict whether customers will churn or not. "
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 1068,
    "start_time": "2021-07-03T03:55:59.432Z"
   },
   {
    "duration": 161,
    "start_time": "2021-07-03T03:56:01.348Z"
   },
   {
    "duration": 7,
    "start_time": "2021-07-03T03:56:02.636Z"
   },
   {
    "duration": 99,
    "start_time": "2021-07-03T03:56:04.249Z"
   },
   {
    "duration": 12159,
    "start_time": "2021-07-03T03:56:08.882Z"
   },
   {
    "duration": 35,
    "start_time": "2021-07-03T03:56:21.042Z"
   },
   {
    "duration": 6,
    "start_time": "2021-07-03T03:56:21.079Z"
   },
   {
    "duration": 23,
    "start_time": "2021-07-03T03:56:21.086Z"
   },
   {
    "duration": 16,
    "start_time": "2021-07-03T03:56:21.110Z"
   },
   {
    "duration": 536,
    "start_time": "2021-07-03T03:56:26.196Z"
   },
   {
    "duration": 4,
    "start_time": "2021-07-03T03:56:31.159Z"
   },
   {
    "duration": 714,
    "start_time": "2021-07-03T03:56:32.042Z"
   },
   {
    "duration": 699,
    "start_time": "2021-07-03T03:56:32.843Z"
   },
   {
    "duration": 312,
    "start_time": "2021-07-03T03:56:43.134Z"
   },
   {
    "duration": 513,
    "start_time": "2021-07-03T03:56:43.448Z"
   },
   {
    "duration": 264,
    "start_time": "2021-07-03T03:56:43.963Z"
   },
   {
    "duration": 612,
    "start_time": "2021-07-03T03:56:44.229Z"
   },
   {
    "duration": 98,
    "start_time": "2021-07-03T03:56:44.844Z"
   },
   {
    "duration": 141238,
    "start_time": "2021-07-03T03:56:56.627Z"
   },
   {
    "duration": 5,
    "start_time": "2021-07-03T03:59:17.868Z"
   },
   {
    "duration": 421,
    "start_time": "2021-07-03T03:59:17.875Z"
   },
   {
    "duration": 4,
    "start_time": "2021-07-03T03:59:40.139Z"
   },
   {
    "duration": 408,
    "start_time": "2021-07-03T03:59:41.020Z"
   },
   {
    "duration": -1587,
    "start_time": "2021-07-03T04:09:15.763Z"
   },
   {
    "duration": -1826,
    "start_time": "2021-07-03T04:09:16.003Z"
   },
   {
    "duration": -1830,
    "start_time": "2021-07-03T04:09:16.007Z"
   },
   {
    "duration": -1831,
    "start_time": "2021-07-03T04:09:16.009Z"
   },
   {
    "duration": -1833,
    "start_time": "2021-07-03T04:09:16.012Z"
   },
   {
    "duration": -1834,
    "start_time": "2021-07-03T04:09:16.014Z"
   },
   {
    "duration": 7,
    "start_time": "2021-07-03T04:09:18.664Z"
   },
   {
    "duration": 93,
    "start_time": "2021-07-03T04:09:20.591Z"
   },
   {
    "duration": 1031,
    "start_time": "2021-07-03T04:09:40.316Z"
   },
   {
    "duration": 83,
    "start_time": "2021-07-03T04:09:41.349Z"
   },
   {
    "duration": 6,
    "start_time": "2021-07-03T04:09:44.720Z"
   },
   {
    "duration": 85,
    "start_time": "2021-07-03T04:09:45.501Z"
   },
   {
    "duration": 12217,
    "start_time": "2021-07-03T04:09:51.495Z"
   },
   {
    "duration": 41,
    "start_time": "2021-07-03T04:10:03.714Z"
   },
   {
    "duration": 6,
    "start_time": "2021-07-03T04:10:03.757Z"
   },
   {
    "duration": 14,
    "start_time": "2021-07-03T04:10:03.764Z"
   },
   {
    "duration": 9,
    "start_time": "2021-07-03T04:10:03.779Z"
   },
   {
    "duration": 515,
    "start_time": "2021-07-03T04:10:51.649Z"
   },
   {
    "duration": 8,
    "start_time": "2021-07-03T04:12:01.058Z"
   },
   {
    "duration": 67,
    "start_time": "2021-07-03T04:12:01.560Z"
   },
   {
    "duration": 6,
    "start_time": "2021-07-03T04:12:02.323Z"
   },
   {
    "duration": 86,
    "start_time": "2021-07-03T04:12:02.858Z"
   },
   {
    "duration": 12414,
    "start_time": "2021-07-03T04:12:05.339Z"
   },
   {
    "duration": 32,
    "start_time": "2021-07-03T04:12:17.755Z"
   },
   {
    "duration": 5,
    "start_time": "2021-07-03T04:12:17.789Z"
   },
   {
    "duration": 11,
    "start_time": "2021-07-03T04:12:17.796Z"
   },
   {
    "duration": 8,
    "start_time": "2021-07-03T04:12:17.808Z"
   },
   {
    "duration": 440,
    "start_time": "2021-07-03T04:12:23.643Z"
   },
   {
    "duration": 12,
    "start_time": "2021-07-03T04:12:48.088Z"
   },
   {
    "duration": 469,
    "start_time": "2021-07-03T04:13:21.081Z"
   },
   {
    "duration": 247,
    "start_time": "2021-07-03T04:15:27.325Z"
   },
   {
    "duration": 12,
    "start_time": "2021-07-03T04:15:44.773Z"
   },
   {
    "duration": 54,
    "start_time": "2021-07-03T04:16:30.220Z"
   },
   {
    "duration": 1162,
    "start_time": "2021-07-03T04:17:00.420Z"
   },
   {
    "duration": 80,
    "start_time": "2021-07-03T04:17:02.112Z"
   },
   {
    "duration": 6,
    "start_time": "2021-07-03T04:17:03.013Z"
   },
   {
    "duration": 90,
    "start_time": "2021-07-03T04:17:03.651Z"
   },
   {
    "duration": 12064,
    "start_time": "2021-07-03T04:17:13.073Z"
   },
   {
    "duration": 33,
    "start_time": "2021-07-03T04:17:25.141Z"
   },
   {
    "duration": 6,
    "start_time": "2021-07-03T04:17:25.176Z"
   },
   {
    "duration": 12,
    "start_time": "2021-07-03T04:17:25.183Z"
   },
   {
    "duration": 20,
    "start_time": "2021-07-03T04:17:25.196Z"
   },
   {
    "duration": 11,
    "start_time": "2021-07-03T04:19:18.333Z"
   },
   {
    "duration": 54,
    "start_time": "2021-07-03T04:21:28.706Z"
   },
   {
    "duration": 4,
    "start_time": "2021-07-03T04:24:29.885Z"
   },
   {
    "duration": 24,
    "start_time": "2021-07-03T04:24:32.255Z"
   },
   {
    "duration": 312,
    "start_time": "2021-07-03T04:24:38.136Z"
   },
   {
    "duration": 19,
    "start_time": "2021-07-03T04:24:46.825Z"
   },
   {
    "duration": 317,
    "start_time": "2021-07-03T04:25:23.531Z"
   },
   {
    "duration": 12,
    "start_time": "2021-07-03T04:25:52.668Z"
   },
   {
    "duration": 29,
    "start_time": "2021-07-03T04:25:53.115Z"
   },
   {
    "duration": 6,
    "start_time": "2021-07-03T04:26:32.041Z"
   },
   {
    "duration": 6441,
    "start_time": "2021-07-03T04:26:35.376Z"
   },
   {
    "duration": 4,
    "start_time": "2021-07-03T04:26:41.818Z"
   },
   {
    "duration": 41,
    "start_time": "2021-07-03T04:26:41.824Z"
   },
   {
    "duration": 109474,
    "start_time": "2021-07-03T04:29:16.075Z"
   },
   {
    "duration": 4,
    "start_time": "2021-07-03T04:31:05.642Z"
   },
   {
    "duration": 298,
    "start_time": "2021-07-03T04:31:05.648Z"
   },
   {
    "duration": 383467,
    "start_time": "2021-07-03T04:31:23.220Z"
   },
   {
    "duration": 6,
    "start_time": "2021-07-03T04:37:46.689Z"
   },
   {
    "duration": 2340,
    "start_time": "2021-07-03T04:37:46.697Z"
   },
   {
    "duration": 2740925,
    "start_time": "2021-07-03T04:42:09.505Z"
   },
   {
    "duration": 8,
    "start_time": "2021-07-03T05:27:50.432Z"
   },
   {
    "duration": 642,
    "start_time": "2021-07-03T05:27:50.442Z"
   },
   {
    "duration": 132,
    "start_time": "2021-07-03T05:27:51.086Z"
   },
   {
    "duration": -276,
    "start_time": "2021-07-03T05:27:51.495Z"
   },
   {
    "duration": -282,
    "start_time": "2021-07-03T05:27:51.502Z"
   },
   {
    "duration": 376,
    "start_time": "2021-07-03T05:28:21.303Z"
   },
   {
    "duration": 5,
    "start_time": "2021-07-03T05:28:22.448Z"
   },
   {
    "duration": 296,
    "start_time": "2021-07-03T05:31:36.920Z"
   },
   {
    "duration": -979,
    "start_time": "2021-07-03T08:21:16.229Z"
   },
   {
    "duration": -986,
    "start_time": "2021-07-03T08:21:16.237Z"
   },
   {
    "duration": -987,
    "start_time": "2021-07-03T08:21:16.240Z"
   },
   {
    "duration": 42,
    "start_time": "2021-07-03T08:32:51.150Z"
   },
   {
    "duration": 20063,
    "start_time": "2021-07-03T08:33:28.060Z"
   },
   {
    "duration": 7,
    "start_time": "2021-07-03T08:45:59.944Z"
   },
   {
    "duration": 267,
    "start_time": "2021-07-03T08:46:00.906Z"
   },
   {
    "duration": 5,
    "start_time": "2021-07-03T08:48:39.075Z"
   },
   {
    "duration": 19012,
    "start_time": "2021-07-03T08:48:39.712Z"
   },
   {
    "duration": 7,
    "start_time": "2021-07-03T08:49:46.079Z"
   },
   {
    "duration": 18517,
    "start_time": "2021-07-03T08:49:46.598Z"
   },
   {
    "duration": 527,
    "start_time": "2021-07-03T08:51:19.501Z"
   },
   {
    "duration": 7,
    "start_time": "2021-07-03T08:54:01.368Z"
   },
   {
    "duration": 1076,
    "start_time": "2021-07-03T08:54:08.586Z"
   },
   {
    "duration": 7,
    "start_time": "2021-07-03T08:56:29.106Z"
   },
   {
    "duration": 578,
    "start_time": "2021-07-03T08:56:35.789Z"
   },
   {
    "duration": 499,
    "start_time": "2021-07-03T08:57:17.525Z"
   },
   {
    "duration": 548,
    "start_time": "2021-07-03T09:05:09.139Z"
   },
   {
    "duration": 6,
    "start_time": "2021-07-03T09:05:36.597Z"
   },
   {
    "duration": 21948,
    "start_time": "2021-07-03T09:05:37.852Z"
   },
   {
    "duration": 6,
    "start_time": "2021-07-03T09:06:54.202Z"
   },
   {
    "duration": 7,
    "start_time": "2021-07-03T09:07:04.223Z"
   },
   {
    "duration": 20313,
    "start_time": "2021-07-03T09:07:05.391Z"
   },
   {
    "duration": 17,
    "start_time": "2021-07-03T09:21:26.727Z"
   },
   {
    "duration": 93,
    "start_time": "2021-07-03T09:26:27.847Z"
   },
   {
    "duration": 94,
    "start_time": "2021-07-03T09:27:34.249Z"
   },
   {
    "duration": 7,
    "start_time": "2021-07-03T09:28:08.311Z"
   },
   {
    "duration": 18996,
    "start_time": "2021-07-03T09:28:12.892Z"
   },
   {
    "duration": 4,
    "start_time": "2021-07-03T09:36:42.989Z"
   },
   {
    "duration": 69,
    "start_time": "2021-07-03T09:36:43.471Z"
   },
   {
    "duration": 17,
    "start_time": "2021-07-03T09:37:30.561Z"
   },
   {
    "duration": 25,
    "start_time": "2021-07-03T09:37:48.338Z"
   },
   {
    "duration": 10,
    "start_time": "2021-07-03T10:04:00.418Z"
   },
   {
    "duration": 1149,
    "start_time": "2021-07-04T00:29:32.266Z"
   },
   {
    "duration": 216,
    "start_time": "2021-07-04T00:29:40.781Z"
   },
   {
    "duration": 6,
    "start_time": "2021-07-04T00:29:41.340Z"
   },
   {
    "duration": 84,
    "start_time": "2021-07-04T00:29:41.999Z"
   },
   {
    "duration": 12401,
    "start_time": "2021-07-04T00:29:49.841Z"
   },
   {
    "duration": 33,
    "start_time": "2021-07-04T00:30:02.244Z"
   },
   {
    "duration": 16,
    "start_time": "2021-07-04T00:30:02.279Z"
   },
   {
    "duration": 12,
    "start_time": "2021-07-04T00:30:02.297Z"
   },
   {
    "duration": 11,
    "start_time": "2021-07-04T00:30:02.310Z"
   },
   {
    "duration": 10,
    "start_time": "2021-07-04T00:30:02.323Z"
   },
   {
    "duration": 70,
    "start_time": "2021-07-04T00:30:02.335Z"
   },
   {
    "duration": 78,
    "start_time": "2021-07-04T00:48:13.947Z"
   },
   {
    "duration": 4,
    "start_time": "2021-07-04T00:51:41.132Z"
   },
   {
    "duration": 23,
    "start_time": "2021-07-04T00:53:22.824Z"
   },
   {
    "duration": 1089,
    "start_time": "2021-07-04T00:55:10.752Z"
   },
   {
    "duration": 112,
    "start_time": "2021-07-04T00:55:13.174Z"
   },
   {
    "duration": 7,
    "start_time": "2021-07-04T00:55:13.768Z"
   },
   {
    "duration": 85,
    "start_time": "2021-07-04T00:55:14.406Z"
   },
   {
    "duration": 11972,
    "start_time": "2021-07-04T00:55:17.690Z"
   },
   {
    "duration": 42,
    "start_time": "2021-07-04T00:55:29.664Z"
   },
   {
    "duration": 6,
    "start_time": "2021-07-04T00:55:29.707Z"
   },
   {
    "duration": 17,
    "start_time": "2021-07-04T00:55:29.715Z"
   },
   {
    "duration": 8,
    "start_time": "2021-07-04T00:55:29.734Z"
   },
   {
    "duration": 21,
    "start_time": "2021-07-04T00:55:29.743Z"
   },
   {
    "duration": 54,
    "start_time": "2021-07-04T00:55:29.765Z"
   },
   {
    "duration": 5,
    "start_time": "2021-07-04T00:55:37.464Z"
   },
   {
    "duration": 24,
    "start_time": "2021-07-04T00:55:40.216Z"
   },
   {
    "duration": 266,
    "start_time": "2021-07-04T00:56:09.435Z"
   },
   {
    "duration": 18,
    "start_time": "2021-07-04T00:58:01.416Z"
   },
   {
    "duration": 274,
    "start_time": "2021-07-04T00:58:12.364Z"
   },
   {
    "duration": 11,
    "start_time": "2021-07-04T00:59:42.129Z"
   },
   {
    "duration": 28,
    "start_time": "2021-07-04T00:59:48.628Z"
   },
   {
    "duration": 7,
    "start_time": "2021-07-04T01:00:30.556Z"
   },
   {
    "duration": 6807,
    "start_time": "2021-07-04T01:32:23.716Z"
   },
   {
    "duration": 5,
    "start_time": "2021-07-04T01:46:17.748Z"
   },
   {
    "duration": 549,
    "start_time": "2021-07-04T01:46:20.324Z"
   },
   {
    "duration": 5,
    "start_time": "2021-07-04T01:47:09.179Z"
   },
   {
    "duration": 460,
    "start_time": "2021-07-04T01:47:10.348Z"
   },
   {
    "duration": 5,
    "start_time": "2021-07-04T01:53:48.977Z"
   },
   {
    "duration": 23,
    "start_time": "2021-07-04T01:53:49.706Z"
   },
   {
    "duration": 5,
    "start_time": "2021-07-04T01:56:53.709Z"
   },
   {
    "duration": 24,
    "start_time": "2021-07-04T01:56:54.035Z"
   },
   {
    "duration": 116622,
    "start_time": "2021-07-04T02:03:45.548Z"
   },
   {
    "duration": 105144,
    "start_time": "2021-07-04T02:09:16.544Z"
   },
   {
    "duration": 253,
    "start_time": "2021-07-04T02:24:51.354Z"
   },
   {
    "duration": 112694,
    "start_time": "2021-07-04T02:25:14.075Z"
   },
   {
    "duration": 109082,
    "start_time": "2021-07-04T02:31:16.612Z"
   },
   {
    "duration": 6,
    "start_time": "2021-07-04T02:38:25.631Z"
   },
   {
    "duration": 134,
    "start_time": "2021-07-04T02:38:26.269Z"
   },
   {
    "duration": 6,
    "start_time": "2021-07-04T02:39:15.705Z"
   },
   {
    "duration": 187,
    "start_time": "2021-07-04T02:39:17.005Z"
   },
   {
    "duration": 1603,
    "start_time": "2021-07-04T06:25:16.008Z"
   },
   {
    "duration": 5172,
    "start_time": "2021-07-04T06:25:46.367Z"
   },
   {
    "duration": 14,
    "start_time": "2021-07-04T06:26:03.020Z"
   },
   {
    "duration": 1649,
    "start_time": "2021-07-04T06:26:15.738Z"
   },
   {
    "duration": 206,
    "start_time": "2021-07-04T06:26:17.390Z"
   },
   {
    "duration": 11,
    "start_time": "2021-07-04T06:26:17.599Z"
   },
   {
    "duration": 181,
    "start_time": "2021-07-04T06:26:17.612Z"
   },
   {
    "duration": 21979,
    "start_time": "2021-07-04T06:26:17.795Z"
   },
   {
    "duration": 65,
    "start_time": "2021-07-04T06:26:39.777Z"
   },
   {
    "duration": 11,
    "start_time": "2021-07-04T06:26:39.844Z"
   },
   {
    "duration": 48,
    "start_time": "2021-07-04T06:26:39.859Z"
   },
   {
    "duration": 14,
    "start_time": "2021-07-04T06:26:39.911Z"
   },
   {
    "duration": 23,
    "start_time": "2021-07-04T06:26:39.929Z"
   },
   {
    "duration": 135,
    "start_time": "2021-07-04T06:26:39.987Z"
   },
   {
    "duration": 8,
    "start_time": "2021-07-04T06:26:40.125Z"
   },
   {
    "duration": 154,
    "start_time": "2021-07-04T06:26:40.135Z"
   },
   {
    "duration": 485,
    "start_time": "2021-07-04T06:26:40.293Z"
   },
   {
    "duration": 107,
    "start_time": "2021-07-04T06:26:40.781Z"
   },
   {
    "duration": 476,
    "start_time": "2021-07-04T06:26:40.892Z"
   },
   {
    "duration": 26,
    "start_time": "2021-07-04T06:26:41.371Z"
   },
   {
    "duration": 89,
    "start_time": "2021-07-04T06:26:41.400Z"
   },
   {
    "duration": 101,
    "start_time": "2021-07-04T06:26:41.493Z"
   },
   {
    "duration": 10697,
    "start_time": "2021-07-04T06:26:41.597Z"
   },
   {
    "duration": 8,
    "start_time": "2021-07-04T06:26:52.296Z"
   },
   {
    "duration": 44,
    "start_time": "2021-07-04T06:26:52.306Z"
   },
   {
    "duration": 209036,
    "start_time": "2021-07-04T06:26:52.355Z"
   },
   {
    "duration": 10,
    "start_time": "2021-07-04T06:30:21.395Z"
   },
   {
    "duration": 178,
    "start_time": "2021-07-04T06:30:21.408Z"
   },
   {
    "duration": 635073,
    "start_time": "2021-07-04T06:30:21.589Z"
   },
   {
    "duration": 9,
    "start_time": "2021-07-04T06:40:56.665Z"
   },
   {
    "duration": 3750,
    "start_time": "2021-07-04T06:40:56.686Z"
   },
   {
    "duration": 4680130,
    "start_time": "2021-07-04T06:41:00.439Z"
   },
   {
    "duration": 17,
    "start_time": "2021-07-04T07:59:00.572Z"
   },
   {
    "duration": 1195,
    "start_time": "2021-07-04T07:59:00.591Z"
   },
   {
    "duration": 6,
    "start_time": "2021-07-04T07:59:01.788Z"
   },
   {
    "duration": 24678,
    "start_time": "2021-07-04T07:59:01.797Z"
   },
   {
    "duration": 17,
    "start_time": "2021-07-04T08:00:19.872Z"
   },
   {
    "duration": 26872,
    "start_time": "2021-07-04T08:00:19.891Z"
   },
   {
    "duration": 11,
    "start_time": "2021-07-04T08:07:48.231Z"
   },
   {
    "duration": 27000,
    "start_time": "2021-07-04T08:07:49.789Z"
   },
   {
    "duration": 13,
    "start_time": "2021-07-04T08:09:15.086Z"
   },
   {
    "duration": 857,
    "start_time": "2021-07-04T08:09:15.807Z"
   },
   {
    "duration": 12,
    "start_time": "2021-07-04T08:50:35.492Z"
   },
   {
    "duration": 1910,
    "start_time": "2021-07-04T08:50:35.993Z"
   },
   {
    "duration": 11,
    "start_time": "2021-07-04T08:50:59.312Z"
   },
   {
    "duration": 1675,
    "start_time": "2021-07-04T08:50:59.947Z"
   },
   {
    "duration": 12,
    "start_time": "2021-07-04T08:51:32.070Z"
   },
   {
    "duration": 1898,
    "start_time": "2021-07-04T08:51:33.001Z"
   },
   {
    "duration": 11,
    "start_time": "2021-07-04T08:52:59.658Z"
   },
   {
    "duration": 1796,
    "start_time": "2021-07-04T08:53:00.779Z"
   },
   {
    "duration": 10219,
    "start_time": "2021-07-04T09:13:30.715Z"
   },
   {
    "duration": 9966,
    "start_time": "2021-07-04T09:14:44.597Z"
   },
   {
    "duration": 11148,
    "start_time": "2021-07-04T09:15:04.374Z"
   },
   {
    "duration": 20135,
    "start_time": "2021-07-04T09:19:55.338Z"
   },
   {
    "duration": 19520,
    "start_time": "2021-07-04T09:22:24.496Z"
   },
   {
    "duration": 12,
    "start_time": "2021-07-04T09:25:44.069Z"
   },
   {
    "duration": 1855,
    "start_time": "2021-07-04T09:25:46.930Z"
   },
   {
    "duration": 94,
    "start_time": "2021-07-04T09:26:54.023Z"
   },
   {
    "duration": 1869,
    "start_time": "2021-07-04T09:26:54.535Z"
   },
   {
    "duration": 12,
    "start_time": "2021-07-04T09:27:17.276Z"
   },
   {
    "duration": 2554,
    "start_time": "2021-07-04T09:27:17.797Z"
   },
   {
    "duration": 11,
    "start_time": "2021-07-04T09:27:36.612Z"
   },
   {
    "duration": 5333,
    "start_time": "2021-07-04T09:27:37.133Z"
   },
   {
    "duration": 12,
    "start_time": "2021-07-04T09:28:17.979Z"
   },
   {
    "duration": 1615,
    "start_time": "2021-07-04T09:28:18.331Z"
   },
   {
    "duration": 7,
    "start_time": "2021-07-04T09:32:44.082Z"
   },
   {
    "duration": 26316,
    "start_time": "2021-07-04T09:32:44.573Z"
   },
   {
    "duration": 6,
    "start_time": "2021-07-04T09:33:33.164Z"
   },
   {
    "duration": 94527,
    "start_time": "2021-07-04T09:33:34.016Z"
   },
   {
    "duration": 7,
    "start_time": "2021-07-04T09:36:02.715Z"
   },
   {
    "duration": 27394,
    "start_time": "2021-07-04T09:36:03.810Z"
   },
   {
    "duration": 1796,
    "start_time": "2021-07-06T17:54:40.904Z"
   },
   {
    "duration": 200,
    "start_time": "2021-07-06T17:54:45.000Z"
   },
   {
    "duration": 11,
    "start_time": "2021-07-06T17:54:46.037Z"
   },
   {
    "duration": 149,
    "start_time": "2021-07-06T17:54:47.799Z"
   },
   {
    "duration": 23443,
    "start_time": "2021-07-06T17:54:50.351Z"
   },
   {
    "duration": 77,
    "start_time": "2021-07-06T17:55:13.797Z"
   },
   {
    "duration": 11,
    "start_time": "2021-07-06T17:55:13.877Z"
   },
   {
    "duration": 16,
    "start_time": "2021-07-06T17:55:45.138Z"
   },
   {
    "duration": 12,
    "start_time": "2021-07-06T17:55:46.002Z"
   },
   {
    "duration": 22,
    "start_time": "2021-07-06T17:55:48.760Z"
   },
   {
    "duration": 92,
    "start_time": "2021-07-06T17:55:50.635Z"
   },
   {
    "duration": 6,
    "start_time": "2021-07-06T17:56:11.381Z"
   },
   {
    "duration": 285,
    "start_time": "2021-07-06T17:56:14.573Z"
   },
   {
    "duration": 516,
    "start_time": "2021-07-06T17:56:18.331Z"
   },
   {
    "duration": 128,
    "start_time": "2021-07-06T17:56:29.150Z"
   },
   {
    "duration": 452,
    "start_time": "2021-07-06T17:56:32.611Z"
   },
   {
    "duration": 20,
    "start_time": "2021-07-06T17:56:45.253Z"
   },
   {
    "duration": 145,
    "start_time": "2021-07-06T17:56:46.049Z"
   },
   {
    "duration": 11,
    "start_time": "2021-07-06T17:56:50.386Z"
   },
   {
    "duration": 1576,
    "start_time": "2021-07-06T18:06:22.730Z"
   },
   {
    "duration": 118,
    "start_time": "2021-07-06T18:06:24.309Z"
   },
   {
    "duration": 10,
    "start_time": "2021-07-06T18:06:24.430Z"
   },
   {
    "duration": 167,
    "start_time": "2021-07-06T18:06:24.443Z"
   },
   {
    "duration": 23956,
    "start_time": "2021-07-06T18:06:24.612Z"
   },
   {
    "duration": 88,
    "start_time": "2021-07-06T18:06:48.571Z"
   },
   {
    "duration": 10,
    "start_time": "2021-07-06T18:06:48.661Z"
   },
   {
    "duration": 17,
    "start_time": "2021-07-06T18:06:48.676Z"
   },
   {
    "duration": 13,
    "start_time": "2021-07-06T18:06:48.696Z"
   },
   {
    "duration": 54,
    "start_time": "2021-07-06T18:06:48.712Z"
   },
   {
    "duration": 105,
    "start_time": "2021-07-06T18:06:48.769Z"
   },
   {
    "duration": 8,
    "start_time": "2021-07-06T18:06:48.877Z"
   },
   {
    "duration": 169,
    "start_time": "2021-07-06T18:06:48.887Z"
   },
   {
    "duration": 504,
    "start_time": "2021-07-06T18:06:49.059Z"
   },
   {
    "duration": 32,
    "start_time": "2021-07-06T18:06:49.565Z"
   },
   {
    "duration": 529,
    "start_time": "2021-07-06T18:06:49.658Z"
   },
   {
    "duration": 22,
    "start_time": "2021-07-06T18:06:50.190Z"
   },
   {
    "duration": 148,
    "start_time": "2021-07-06T18:06:50.215Z"
   },
   {
    "duration": 97,
    "start_time": "2021-07-06T18:06:50.366Z"
   },
   {
    "duration": 11886,
    "start_time": "2021-07-06T18:06:50.465Z"
   },
   {
    "duration": 8,
    "start_time": "2021-07-06T18:07:02.357Z"
   },
   {
    "duration": 47,
    "start_time": "2021-07-06T18:07:02.368Z"
   },
   {
    "duration": 175444,
    "start_time": "2021-07-06T18:07:02.420Z"
   },
   {
    "duration": 9,
    "start_time": "2021-07-06T18:09:57.868Z"
   },
   {
    "duration": 1779,
    "start_time": "2021-07-06T18:09:57.879Z"
   },
   {
    "duration": 656024,
    "start_time": "2021-07-06T18:09:59.661Z"
   },
   {
    "duration": 9,
    "start_time": "2021-07-06T18:20:55.687Z"
   },
   {
    "duration": 3799,
    "start_time": "2021-07-06T18:20:55.699Z"
   },
   {
    "duration": 4782737,
    "start_time": "2021-07-06T18:20:59.501Z"
   },
   {
    "duration": 17,
    "start_time": "2021-07-06T19:40:42.240Z"
   },
   {
    "duration": 1103,
    "start_time": "2021-07-06T19:40:42.260Z"
   },
   {
    "duration": 27,
    "start_time": "2021-07-06T19:40:43.380Z"
   },
   {
    "duration": 41881,
    "start_time": "2021-07-06T19:40:43.417Z"
   },
   {
    "duration": 10139,
    "start_time": "2021-07-06T19:41:25.300Z"
   },
   {
    "duration": 21144,
    "start_time": "2021-07-06T19:41:35.442Z"
   },
   {
    "duration": 11,
    "start_time": "2021-07-06T19:41:56.588Z"
   },
   {
    "duration": 1582,
    "start_time": "2021-07-06T19:41:56.602Z"
   },
   {
    "duration": 6,
    "start_time": "2021-07-06T19:41:58.186Z"
   },
   {
    "duration": 39823,
    "start_time": "2021-07-06T19:41:58.194Z"
   },
   {
    "duration": 1610,
    "start_time": "2021-07-10T10:32:40.503Z"
   },
   {
    "duration": 100,
    "start_time": "2021-07-10T10:32:47.499Z"
   },
   {
    "duration": 11,
    "start_time": "2021-07-10T10:32:48.320Z"
   },
   {
    "duration": 159,
    "start_time": "2021-07-10T10:32:49.164Z"
   },
   {
    "duration": 22679,
    "start_time": "2021-07-10T10:34:43.619Z"
   },
   {
    "duration": 66,
    "start_time": "2021-07-10T10:35:06.300Z"
   },
   {
    "duration": 9,
    "start_time": "2021-07-10T10:35:06.369Z"
   },
   {
    "duration": 41,
    "start_time": "2021-07-10T10:35:06.380Z"
   },
   {
    "duration": 15,
    "start_time": "2021-07-10T10:35:06.424Z"
   },
   {
    "duration": 18,
    "start_time": "2021-07-10T10:35:15.960Z"
   },
   {
    "duration": 13,
    "start_time": "2021-07-10T12:30:17.591Z"
   },
   {
    "duration": 149,
    "start_time": "2021-07-10T12:30:23.451Z"
   },
   {
    "duration": 10,
    "start_time": "2021-07-10T12:30:23.603Z"
   },
   {
    "duration": 171,
    "start_time": "2021-07-10T12:30:23.616Z"
   },
   {
    "duration": 22941,
    "start_time": "2021-07-10T12:30:24.330Z"
   },
   {
    "duration": 81,
    "start_time": "2021-07-10T12:30:47.274Z"
   },
   {
    "duration": 10,
    "start_time": "2021-07-10T12:30:47.358Z"
   },
   {
    "duration": 38,
    "start_time": "2021-07-10T12:30:47.371Z"
   },
   {
    "duration": 14,
    "start_time": "2021-07-10T12:30:47.412Z"
   },
   {
    "duration": 19,
    "start_time": "2021-07-10T12:30:47.428Z"
   },
   {
    "duration": 544,
    "start_time": "2021-07-10T12:30:48.927Z"
   },
   {
    "duration": 96,
    "start_time": "2021-07-10T12:32:16.964Z"
   },
   {
    "duration": 12,
    "start_time": "2021-07-10T12:32:34.641Z"
   },
   {
    "duration": 167,
    "start_time": "2021-07-10T12:32:36.853Z"
   },
   {
    "duration": 419,
    "start_time": "2021-07-10T12:32:39.307Z"
   },
   {
    "duration": 89,
    "start_time": "2021-07-10T12:32:44.523Z"
   },
   {
    "duration": 377,
    "start_time": "2021-07-10T12:32:44.934Z"
   },
   {
    "duration": 16,
    "start_time": "2021-07-10T12:32:49.908Z"
   },
   {
    "duration": 74,
    "start_time": "2021-07-10T12:32:50.138Z"
   },
   {
    "duration": 11,
    "start_time": "2021-07-10T12:32:55.671Z"
   },
   {
    "duration": 11470,
    "start_time": "2021-07-10T12:32:57.771Z"
   },
   {
    "duration": 10,
    "start_time": "2021-07-10T12:33:09.245Z"
   },
   {
    "duration": 60,
    "start_time": "2021-07-10T12:33:09.259Z"
   },
   {
    "duration": 202796,
    "start_time": "2021-07-10T12:33:09.322Z"
   },
   {
    "duration": 9,
    "start_time": "2021-07-10T12:36:32.121Z"
   },
   {
    "duration": 277,
    "start_time": "2021-07-10T12:36:32.133Z"
   },
   {
    "duration": 636647,
    "start_time": "2021-07-10T12:36:32.414Z"
   },
   {
    "duration": 8,
    "start_time": "2021-07-10T12:47:09.065Z"
   },
   {
    "duration": 433,
    "start_time": "2021-07-10T12:47:09.076Z"
   },
   {
    "duration": 1597,
    "start_time": "2021-07-10T13:09:03.487Z"
   },
   {
    "duration": 83,
    "start_time": "2021-07-10T13:09:09.367Z"
   },
   {
    "duration": 11,
    "start_time": "2021-07-10T13:09:09.862Z"
   },
   {
    "duration": 232,
    "start_time": "2021-07-10T13:09:10.515Z"
   },
   {
    "duration": 34337,
    "start_time": "2021-07-10T13:09:20.622Z"
   },
   {
    "duration": 93,
    "start_time": "2021-07-10T13:09:54.962Z"
   },
   {
    "duration": 13,
    "start_time": "2021-07-10T13:09:55.058Z"
   },
   {
    "duration": 50,
    "start_time": "2021-07-10T13:09:55.074Z"
   },
   {
    "duration": 16,
    "start_time": "2021-07-10T13:09:55.127Z"
   },
   {
    "duration": 26,
    "start_time": "2021-07-10T13:09:55.146Z"
   },
   {
    "duration": 113,
    "start_time": "2021-07-10T13:11:37.716Z"
   },
   {
    "duration": 1677,
    "start_time": "2021-07-10T13:26:02.293Z"
   },
   {
    "duration": 71,
    "start_time": "2021-07-10T13:26:05.271Z"
   },
   {
    "duration": 12,
    "start_time": "2021-07-10T13:26:05.895Z"
   },
   {
    "duration": 171,
    "start_time": "2021-07-10T13:26:06.471Z"
   },
   {
    "duration": 22998,
    "start_time": "2021-07-10T13:26:09.188Z"
   },
   {
    "duration": 89,
    "start_time": "2021-07-10T13:26:32.189Z"
   },
   {
    "duration": 30,
    "start_time": "2021-07-10T13:26:32.281Z"
   },
   {
    "duration": 24,
    "start_time": "2021-07-10T13:26:32.314Z"
   },
   {
    "duration": 20,
    "start_time": "2021-07-10T13:26:32.341Z"
   },
   {
    "duration": 18,
    "start_time": "2021-07-10T13:26:32.363Z"
   },
   {
    "duration": 76,
    "start_time": "2021-07-10T13:26:32.409Z"
   },
   {
    "duration": 8,
    "start_time": "2021-07-10T13:31:13.886Z"
   },
   {
    "duration": 5,
    "start_time": "2021-07-10T13:39:52.086Z"
   },
   {
    "duration": 1160,
    "start_time": "2021-07-10T13:41:49.556Z"
   },
   {
    "duration": 7,
    "start_time": "2021-07-10T13:45:53.118Z"
   },
   {
    "duration": 725,
    "start_time": "2021-07-10T13:46:13.569Z"
   },
   {
    "duration": 35,
    "start_time": "2021-07-10T13:46:49.857Z"
   },
   {
    "duration": 7,
    "start_time": "2021-07-10T13:51:26.484Z"
   },
   {
    "duration": 6,
    "start_time": "2021-07-10T13:51:30.326Z"
   },
   {
    "duration": 35,
    "start_time": "2021-07-10T13:51:30.766Z"
   },
   {
    "duration": 8,
    "start_time": "2021-07-10T13:52:16.318Z"
   },
   {
    "duration": 5,
    "start_time": "2021-07-10T13:52:19.579Z"
   },
   {
    "duration": 39,
    "start_time": "2021-07-10T13:52:21.450Z"
   },
   {
    "duration": 5,
    "start_time": "2021-07-10T14:01:09.644Z"
   },
   {
    "duration": 115,
    "start_time": "2021-07-10T14:02:41.597Z"
   },
   {
    "duration": 467,
    "start_time": "2021-07-10T14:03:13.681Z"
   },
   {
    "duration": 44,
    "start_time": "2021-07-10T14:06:37.965Z"
   },
   {
    "duration": 368,
    "start_time": "2021-07-10T14:06:42.249Z"
   },
   {
    "duration": 17,
    "start_time": "2021-07-10T14:07:50.554Z"
   },
   {
    "duration": 87,
    "start_time": "2021-07-10T14:08:49.323Z"
   },
   {
    "duration": 12,
    "start_time": "2021-07-10T14:11:19.172Z"
   },
   {
    "duration": 1502,
    "start_time": "2021-07-10T14:12:37.590Z"
   },
   {
    "duration": 129,
    "start_time": "2021-07-10T14:12:39.292Z"
   },
   {
    "duration": 12,
    "start_time": "2021-07-10T14:12:47.007Z"
   },
   {
    "duration": 165,
    "start_time": "2021-07-10T14:12:47.378Z"
   },
   {
    "duration": 23162,
    "start_time": "2021-07-10T14:12:48.620Z"
   },
   {
    "duration": 76,
    "start_time": "2021-07-10T14:13:11.784Z"
   },
   {
    "duration": 9,
    "start_time": "2021-07-10T14:13:11.863Z"
   },
   {
    "duration": 38,
    "start_time": "2021-07-10T14:13:11.875Z"
   },
   {
    "duration": 11,
    "start_time": "2021-07-10T14:13:11.916Z"
   },
   {
    "duration": 21,
    "start_time": "2021-07-10T14:13:11.930Z"
   },
   {
    "duration": 111,
    "start_time": "2021-07-10T14:13:11.954Z"
   },
   {
    "duration": 8,
    "start_time": "2021-07-10T14:13:12.070Z"
   },
   {
    "duration": 12,
    "start_time": "2021-07-10T14:13:12.081Z"
   },
   {
    "duration": 47,
    "start_time": "2021-07-10T14:13:12.095Z"
   },
   {
    "duration": 6,
    "start_time": "2021-07-10T14:13:12.145Z"
   },
   {
    "duration": 259,
    "start_time": "2021-07-10T14:13:12.153Z"
   },
   {
    "duration": 406,
    "start_time": "2021-07-10T14:13:12.415Z"
   },
   {
    "duration": 35,
    "start_time": "2021-07-10T14:13:12.824Z"
   },
   {
    "duration": 422,
    "start_time": "2021-07-10T14:13:12.911Z"
   },
   {
    "duration": 19,
    "start_time": "2021-07-10T14:13:13.335Z"
   },
   {
    "duration": 154,
    "start_time": "2021-07-10T14:13:13.356Z"
   },
   {
    "duration": 99,
    "start_time": "2021-07-10T14:13:13.514Z"
   },
   {
    "duration": 12775,
    "start_time": "2021-07-10T14:14:53.887Z"
   },
   {
    "duration": 8,
    "start_time": "2021-07-10T14:19:35.029Z"
   },
   {
    "duration": 48,
    "start_time": "2021-07-10T14:19:35.533Z"
   },
   {
    "duration": 181757,
    "start_time": "2021-07-10T14:21:08.959Z"
   },
   {
    "duration": 9,
    "start_time": "2021-07-10T14:24:14.435Z"
   },
   {
    "duration": 53,
    "start_time": "2021-07-10T14:24:35.061Z"
   },
   {
    "duration": 1632,
    "start_time": "2021-07-10T14:28:11.984Z"
   },
   {
    "duration": 82,
    "start_time": "2021-07-10T14:28:13.618Z"
   },
   {
    "duration": 11,
    "start_time": "2021-07-10T14:28:13.703Z"
   },
   {
    "duration": 192,
    "start_time": "2021-07-10T14:28:13.717Z"
   },
   {
    "duration": 22932,
    "start_time": "2021-07-10T14:28:14.390Z"
   },
   {
    "duration": 60,
    "start_time": "2021-07-10T14:28:37.325Z"
   },
   {
    "duration": 28,
    "start_time": "2021-07-10T14:28:37.388Z"
   },
   {
    "duration": 16,
    "start_time": "2021-07-10T14:28:37.418Z"
   },
   {
    "duration": 11,
    "start_time": "2021-07-10T14:28:37.437Z"
   },
   {
    "duration": 18,
    "start_time": "2021-07-10T14:28:37.450Z"
   },
   {
    "duration": 78,
    "start_time": "2021-07-10T14:28:37.509Z"
   },
   {
    "duration": 8,
    "start_time": "2021-07-10T14:28:37.609Z"
   },
   {
    "duration": 11,
    "start_time": "2021-07-10T14:28:37.619Z"
   },
   {
    "duration": 33,
    "start_time": "2021-07-10T14:28:39.053Z"
   },
   {
    "duration": 6,
    "start_time": "2021-07-10T14:28:49.043Z"
   },
   {
    "duration": 58,
    "start_time": "2021-07-10T14:28:50.462Z"
   },
   {
    "duration": 397,
    "start_time": "2021-07-10T14:28:51.120Z"
   },
   {
    "duration": 95,
    "start_time": "2021-07-10T14:28:53.114Z"
   },
   {
    "duration": 413,
    "start_time": "2021-07-10T14:28:53.310Z"
   },
   {
    "duration": 17,
    "start_time": "2021-07-10T14:28:53.906Z"
   },
   {
    "duration": 117,
    "start_time": "2021-07-10T14:28:54.292Z"
   },
   {
    "duration": 10,
    "start_time": "2021-07-10T14:28:56.039Z"
   },
   {
    "duration": 12592,
    "start_time": "2021-07-10T14:28:57.546Z"
   },
   {
    "duration": 9,
    "start_time": "2021-07-10T14:29:10.142Z"
   },
   {
    "duration": 81,
    "start_time": "2021-07-10T14:29:10.154Z"
   },
   {
    "duration": 471,
    "start_time": "2021-07-10T14:29:10.237Z"
   },
   {
    "duration": 35,
    "start_time": "2021-07-10T14:29:35.999Z"
   },
   {
    "duration": 201390,
    "start_time": "2021-07-10T14:35:18.921Z"
   },
   {
    "duration": 9,
    "start_time": "2021-07-10T14:38:40.314Z"
   },
   {
    "duration": 584,
    "start_time": "2021-07-10T14:38:40.326Z"
   },
   {
    "duration": 123,
    "start_time": "2021-07-10T14:38:40.914Z"
   },
   {
    "duration": 1573,
    "start_time": "2021-07-10T14:43:58.124Z"
   },
   {
    "duration": 1573,
    "start_time": "2021-07-10T14:44:30.151Z"
   },
   {
    "duration": 84,
    "start_time": "2021-07-10T14:44:31.726Z"
   },
   {
    "duration": 10,
    "start_time": "2021-07-10T14:44:31.813Z"
   },
   {
    "duration": 237,
    "start_time": "2021-07-10T14:44:31.826Z"
   },
   {
    "duration": 34532,
    "start_time": "2021-07-10T14:44:32.410Z"
   },
   {
    "duration": 96,
    "start_time": "2021-07-10T14:45:06.945Z"
   },
   {
    "duration": 12,
    "start_time": "2021-07-10T14:45:07.045Z"
   },
   {
    "duration": 37,
    "start_time": "2021-07-10T14:45:07.060Z"
   },
   {
    "duration": 15,
    "start_time": "2021-07-10T14:45:07.108Z"
   },
   {
    "duration": 18,
    "start_time": "2021-07-10T14:45:07.126Z"
   },
   {
    "duration": 110,
    "start_time": "2021-07-10T14:45:07.146Z"
   },
   {
    "duration": 10,
    "start_time": "2021-07-10T14:45:07.260Z"
   },
   {
    "duration": 19,
    "start_time": "2021-07-10T14:45:07.273Z"
   },
   {
    "duration": 53,
    "start_time": "2021-07-10T14:45:07.309Z"
   },
   {
    "duration": 8,
    "start_time": "2021-07-10T14:45:07.365Z"
   },
   {
    "duration": 139,
    "start_time": "2021-07-10T14:45:07.376Z"
   },
   {
    "duration": 496,
    "start_time": "2021-07-10T14:45:07.518Z"
   },
   {
    "duration": 39,
    "start_time": "2021-07-10T14:45:50.739Z"
   },
   {
    "duration": 465,
    "start_time": "2021-07-10T14:45:51.338Z"
   },
   {
    "duration": 23,
    "start_time": "2021-07-10T14:46:02.683Z"
   },
   {
    "duration": 70,
    "start_time": "2021-07-10T14:46:03.454Z"
   },
   {
    "duration": 16,
    "start_time": "2021-07-10T14:46:34.252Z"
   },
   {
    "duration": 17946,
    "start_time": "2021-07-10T14:46:42.729Z"
   },
   {
    "duration": 10,
    "start_time": "2021-07-10T14:48:10.707Z"
   },
   {
    "duration": 49,
    "start_time": "2021-07-10T14:48:12.292Z"
   },
   {
    "duration": 501,
    "start_time": "2021-07-10T14:48:14.727Z"
   },
   {
    "duration": 9,
    "start_time": "2021-07-10T14:49:32.750Z"
   },
   {
    "duration": 81,
    "start_time": "2021-07-10T14:49:35.105Z"
   },
   {
    "duration": 8,
    "start_time": "2021-07-10T14:49:52.796Z"
   },
   {
    "duration": 83,
    "start_time": "2021-07-10T14:49:53.186Z"
   },
   {
    "duration": 185927,
    "start_time": "2021-07-10T14:52:02.383Z"
   },
   {
    "duration": 9,
    "start_time": "2021-07-10T14:55:08.313Z"
   },
   {
    "duration": 500,
    "start_time": "2021-07-10T14:55:08.325Z"
   },
   {
    "duration": 776922,
    "start_time": "2021-07-10T14:56:53.421Z"
   },
   {
    "duration": -1066,
    "start_time": "2021-07-10T15:09:51.776Z"
   },
   {
    "duration": -1074,
    "start_time": "2021-07-10T15:09:51.787Z"
   },
   {
    "duration": -1075,
    "start_time": "2021-07-10T15:09:51.790Z"
   },
   {
    "duration": 10,
    "start_time": "2021-07-10T15:09:59.045Z"
   },
   {
    "duration": 362,
    "start_time": "2021-07-10T15:09:59.661Z"
   },
   {
    "duration": 9,
    "start_time": "2021-07-10T15:10:28.837Z"
   },
   {
    "duration": 365,
    "start_time": "2021-07-10T15:10:29.828Z"
   },
   {
    "duration": 493,
    "start_time": "2021-07-10T15:10:56.390Z"
   },
   {
    "duration": -1357,
    "start_time": "2021-07-10T16:45:36.358Z"
   },
   {
    "duration": -1361,
    "start_time": "2021-07-10T16:45:36.364Z"
   },
   {
    "duration": -1364,
    "start_time": "2021-07-10T16:45:36.368Z"
   },
   {
    "duration": -1366,
    "start_time": "2021-07-10T16:45:36.372Z"
   },
   {
    "duration": -1369,
    "start_time": "2021-07-10T16:45:36.376Z"
   },
   {
    "duration": 6,
    "start_time": "2021-07-10T16:46:51.545Z"
   },
   {
    "duration": 29064,
    "start_time": "2021-07-10T16:46:53.050Z"
   },
   {
    "duration": 8,
    "start_time": "2021-07-10T16:51:00.616Z"
   },
   {
    "duration": 27925,
    "start_time": "2021-07-10T16:51:01.095Z"
   },
   {
    "duration": 1567,
    "start_time": "2021-07-10T16:58:24.750Z"
   },
   {
    "duration": 161,
    "start_time": "2021-07-10T16:58:26.320Z"
   },
   {
    "duration": 11,
    "start_time": "2021-07-10T16:58:26.485Z"
   },
   {
    "duration": 166,
    "start_time": "2021-07-10T16:58:26.498Z"
   },
   {
    "duration": 23001,
    "start_time": "2021-07-10T16:58:26.666Z"
   },
   {
    "duration": 74,
    "start_time": "2021-07-10T16:58:49.670Z"
   },
   {
    "duration": 9,
    "start_time": "2021-07-10T16:58:49.746Z"
   },
   {
    "duration": 60,
    "start_time": "2021-07-10T16:58:49.759Z"
   },
   {
    "duration": 77,
    "start_time": "2021-07-10T16:58:49.822Z"
   },
   {
    "duration": 36,
    "start_time": "2021-07-10T16:58:49.901Z"
   },
   {
    "duration": 80,
    "start_time": "2021-07-10T16:58:49.939Z"
   },
   {
    "duration": 7,
    "start_time": "2021-07-10T16:58:50.022Z"
   },
   {
    "duration": 27,
    "start_time": "2021-07-10T16:58:50.032Z"
   },
   {
    "duration": 51,
    "start_time": "2021-07-10T16:58:50.062Z"
   },
   {
    "duration": 9,
    "start_time": "2021-07-10T16:58:50.116Z"
   },
   {
    "duration": 95,
    "start_time": "2021-07-10T16:58:50.128Z"
   },
   {
    "duration": 504,
    "start_time": "2021-07-10T16:58:50.311Z"
   },
   {
    "duration": 37,
    "start_time": "2021-07-10T16:58:50.817Z"
   },
   {
    "duration": 437,
    "start_time": "2021-07-10T16:58:50.911Z"
   },
   {
    "duration": 24,
    "start_time": "2021-07-10T16:58:51.351Z"
   },
   {
    "duration": 135,
    "start_time": "2021-07-10T16:58:51.378Z"
   },
   {
    "duration": 102,
    "start_time": "2021-07-10T16:58:51.517Z"
   },
   {
    "duration": 14518,
    "start_time": "2021-07-10T16:58:51.626Z"
   },
   {
    "duration": 9,
    "start_time": "2021-07-10T16:59:06.147Z"
   },
   {
    "duration": 107,
    "start_time": "2021-07-10T16:59:06.159Z"
   },
   {
    "duration": 181943,
    "start_time": "2021-07-10T16:59:06.269Z"
   },
   {
    "duration": 99,
    "start_time": "2021-07-10T17:02:08.215Z"
   },
   {
    "duration": 310,
    "start_time": "2021-07-10T17:02:08.316Z"
   },
   {
    "duration": 701213,
    "start_time": "2021-07-10T17:02:08.629Z"
   },
   {
    "duration": 9,
    "start_time": "2021-07-10T17:13:49.845Z"
   },
   {
    "duration": 507,
    "start_time": "2021-07-10T17:13:49.856Z"
   },
   {
    "duration": 5417574,
    "start_time": "2021-07-10T17:13:50.366Z"
   },
   {
    "duration": 10,
    "start_time": "2021-07-10T18:44:07.943Z"
   },
   {
    "duration": 1481,
    "start_time": "2021-07-10T18:44:07.957Z"
   },
   {
    "duration": -1180,
    "start_time": "2021-07-10T18:44:10.621Z"
   },
   {
    "duration": -1400,
    "start_time": "2021-07-10T18:44:10.843Z"
   },
   {
    "duration": -1401,
    "start_time": "2021-07-10T18:44:10.845Z"
   },
   {
    "duration": -1405,
    "start_time": "2021-07-10T18:44:10.851Z"
   },
   {
    "duration": 15,
    "start_time": "2021-07-10T18:51:27.711Z"
   },
   {
    "duration": 788,
    "start_time": "2021-07-10T18:51:27.736Z"
   },
   {
    "duration": 10,
    "start_time": "2021-07-10T18:51:28.528Z"
   },
   {
    "duration": 47082,
    "start_time": "2021-07-10T18:51:28.542Z"
   },
   {
    "duration": 8,
    "start_time": "2021-07-10T18:54:38.897Z"
   },
   {
    "duration": 44969,
    "start_time": "2021-07-10T18:54:39.309Z"
   }
  ],
  "accelerator": "TPU",
  "colab": {
   "name": "customer_churn_predictML.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
